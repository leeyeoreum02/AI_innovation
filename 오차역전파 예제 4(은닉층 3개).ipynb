{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1+np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime      # datetime.now() 를 이용하여 학습 경과 시간 측정\n",
    "\n",
    "class NeuralNetwork:\n",
    "    \n",
    "    def __init__(self, input_nodes, hidden_1_nodes, hidden_2_nodes, output_nodes, learning_rate):\n",
    "        \n",
    "        self.input_nodes = input_nodes\n",
    "        self.hidden_1_nodes = hidden_1_nodes\n",
    "        self.hidden_2_nodes = hidden_2_nodes\n",
    "        self.output_nodes = output_nodes\n",
    "        \n",
    "        # 은닉층 가중치  W2 = (784 X 100) Xavier/He 방법으로 self.W2 가중치 초기화\n",
    "        self.W2 = np.random.randn(self.input_nodes, self.hidden_1_nodes) / np.sqrt(self.input_nodes/2)\n",
    "        self.b2 = np.random.rand(self.hidden_1_nodes)      \n",
    "        \n",
    "        # 출력층 가중치는 W3 = (100X10)  Xavier/He 방법으로 self.W3 가중치 초기화\n",
    "        self.W3 = np.random.randn(self.hidden_1_nodes, self.hidden_2_nodes) / np.sqrt(self.hidden_1_nodes/2)\n",
    "        self.b3 = np.random.rand(self.hidden_2_nodes)      \n",
    "        \n",
    "        self.W4 = np.random.randn(self.hidden_2_nodes, self.output_nodes) / np.sqrt(self.hidden_2_nodes/2)\n",
    "        self.b4 = np.random.rand(self.output_nodes)\n",
    "                        \n",
    "        self.Z4 = np.zeros([1, output_nodes])\n",
    "        self.A4 = np.zeros([1, output_nodes])\n",
    "            \n",
    "        # 출력층 선형회귀 값 Z3, 출력값 A3 정의 (모두 행렬로 표시)\n",
    "        self.Z3 = np.zeros([1, hidden_2_nodes])\n",
    "        self.A3 = np.zeros([1, hidden_2_nodes])\n",
    "        \n",
    "        # 은닉층 선형회귀 값 Z2, 출력값 A2 정의 (모두 행렬로 표시)\n",
    "        self.Z2 = np.zeros([1, hidden_1_nodes])\n",
    "        self.A2 = np.zeros([1, hidden_1_nodes])\n",
    "        \n",
    "        # 입력층 선형회귀 값 Z1, 출력값 A1 정의 (모두 행렬로 표시)\n",
    "        self.Z1 = np.zeros([1, input_nodes])    \n",
    "        self.A1 = np.zeros([1, input_nodes])       \n",
    "        \n",
    "        # 학습률 learning rate 초기화\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "    def feed_forward(self):  \n",
    "        \n",
    "        delta = 1e-7    # log 무한대 발산 방지\n",
    "        \n",
    "        # 입력층 선형회귀 값 Z1, 출력값 A1 계산\n",
    "        self.Z1 = self.input_data\n",
    "        self.A1 = self.input_data\n",
    "        \n",
    "        # 은닉층 선형회귀 값 Z2, 출력값 A2 계산    \n",
    "        self.Z2 = np.dot(self.A1, self.W2) + self.b2\n",
    "        self.A2 = sigmoid(self.Z2)\n",
    "        \n",
    "        # 출력층 선형회귀 값 Z3, 출력값 A3 계산\n",
    "        self.Z3 = np.dot(self.A2, self.W3) + self.b3\n",
    "        self.A3 = sigmoid(self.Z3)\n",
    "        \n",
    "        self.Z4 = np.dot(self.A3, self.W4) + self.b4\n",
    "        y = self.A4 = sigmoid(self.Z4)\n",
    "        \n",
    "        # cross entropy \n",
    "        return  -np.sum( self.target_data*np.log(y + delta) + (1-self.target_data)*np.log((1 - y)+delta ) )    \n",
    "    \n",
    "    def loss_val(self):\n",
    "\n",
    "        delta = 1e-7    # log 무한대 발산 방지\n",
    "        \n",
    "        # 입력층 선형회귀 값 Z1, 출력값 A1 계산\n",
    "        self.Z1 = self.input_data\n",
    "        self.A1 = self.input_data\n",
    "        \n",
    "        # 은닉층 선형회귀 값 Z2, 출력값 A2 계산    \n",
    "        self.Z2 = np.dot(self.A1, self.W2) + self.b2\n",
    "        self.A2 = sigmoid(self.Z2)\n",
    "        \n",
    "        # 출력층 선형회귀 값 Z3, 출력값 A3 계산\n",
    "        self.Z3 = np.dot(self.A2, self.W3) + self.b3\n",
    "        self.A3 = sigmoid(self.Z3)\n",
    "        \n",
    "        self.Z4 = np.dot(self.A3, self.W4) + self.b4\n",
    "        y = self.A4 = sigmoid(self.Z4)\n",
    "        \n",
    "        # cross entropy \n",
    "        return  -np.sum( self.target_data*np.log(y + delta) + (1-self.target_data)*np.log((1 - y)+delta ) )    \n",
    "   \n",
    "    \n",
    "    # 정확도 측정함수 \n",
    "    def accuracy(self, test_input_data, test_target_data):\n",
    "        \n",
    "        matched_list = []\n",
    "        not_matched_list = []\n",
    "        \n",
    "        \n",
    "        for index in range(len(test_input_data)):\n",
    "                        \n",
    "            label = int(test_target_data[index])\n",
    "                        \n",
    "            # one-hot encoding을 위한 데이터 정규화 (data normalize)\n",
    "            data = (test_input_data[index] / 255.0 * 0.99) + 0.01\n",
    "                  \n",
    "            # predict 를 위해서 vector 을 matrix 로 변환하여 인수로 넘겨줌\n",
    "            predicted_num = self.predict(np.array(data, ndmin=2)) \n",
    "        \n",
    "            if label == predicted_num:\n",
    "                matched_list.append(index)\n",
    "                \n",
    "            else:\n",
    "                \n",
    "                not_matched_list.append(index)\n",
    "        \n",
    "        accuracy_val = (len(matched_list)/(len(test_input_data)))\n",
    "        \n",
    "        return accuracy_val, not_matched_list\n",
    "    \n",
    "    \n",
    "    def train(self, input_data, target_data):   # input_data : 784 개, target_data : 10개\n",
    "        \n",
    "        self.target_data = target_data    \n",
    "        self.input_data = input_data\n",
    "        \n",
    "        # 먼저 feed forward 를 통해서 최종 출력값과 이를 바탕으로 현재의 에러 값 계산\n",
    "        loss_val = self.feed_forward()\n",
    "    \n",
    "        loss_4 = (self.A4-self.target_data) * self.A4 * (1-self.A4)     \n",
    "        \n",
    "        self.W4 = self.W4 - self.learning_rate * np.dot(self.A3.T, loss_4)        \n",
    "        self.b4 = self.b4 - self.learning_rate * loss_4\n",
    "\n",
    "        # 출력층 loss 인 loss_3 구함\n",
    "        loss_3 = np.dot(loss_4, self.W4.T) * self.A3 * (1-self.A3)\n",
    "        \n",
    "        # 출력층 가중치 W3, 출력층 바이어스 b3 업데이트\n",
    "        self.W3 = self.W3 - self.learning_rate * (np.dot(self.A2.T, loss_3))   \n",
    "        \n",
    "        self.b3 = self.b3 - self.learning_rate * loss_3  \n",
    "        \n",
    "        # 은닉층 loss 인 loss_2 구함        \n",
    "        loss_2 = np.dot(loss_3, self.W3.T) * self.A2 * (1-self.A2)\n",
    "        \n",
    "        # 은닉층 가중치 W2, 은닉층 바이어스 b2 업데이트\n",
    "        self.W2 = self.W2 - self.learning_rate * (np.dot(self.A1.T, loss_2))   \n",
    "        \n",
    "        self.b2 = self.b2 - self.learning_rate * loss_2\n",
    "        \n",
    "        \n",
    "    def predict(self, input_data):        # input_data 는 행렬로 입력됨 즉, (1, 784) shape 을 가짐        \n",
    "        \n",
    "        Z2 = np.dot(input_data, self.W2) + self.b2\n",
    "        A2 = sigmoid(Z2)\n",
    "        \n",
    "        Z3 = np.dot(A2, self.W3) + self.b3\n",
    "        A3 = sigmoid(Z3)\n",
    "        \n",
    "        Z4 = np.dot(A3, self.W4) + self.b4\n",
    "        y = A4 = sigmoid(Z4)\n",
    "        \n",
    "        predicted_num = np.argmax(y)\n",
    "    \n",
    "        return predicted_num\n",
    "    \n",
    "    def a(self, target_data):\n",
    "        print(\"self.W4.shape = \", self.W4.shape)\n",
    "        print(\"self.A3,shape = \", self.A3.shape)\n",
    "        print(\"self.A3.T.shape = \", self.A3.T.shape)\n",
    "        print(\"loss_4.shape = \", ((self.A4-target_data) * self.A4 * (1-self.A4)).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_data.shape =  (60000, 785)\n",
      "training_data[0,0] =  5.0 , len(training_data[0]) =  785\n"
     ]
    }
   ],
   "source": [
    "# 0~9 숫자 이미지가 784개의 숫자 (28X28) 로 구성되어 있는 training data 읽어옴\n",
    "\n",
    "try:\n",
    "    training_data = np.loadtxt('./mnist_train.csv', delimiter=',', dtype=np.float32)\n",
    "\n",
    "    print(\"training_data.shape = \", training_data.shape)\n",
    "    print(\"training_data[0,0] = \", training_data[0,0], \", len(training_data[0]) = \", len(training_data[0]))\n",
    "    \n",
    "except Exception as err:\n",
    "    \n",
    "    print('Exception occur !!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epochs =  0 , step =  0 , current loss_val =  9.799139001959393\n",
      "epochs =  0 , step =  1000 , current loss_val =  3.179463067991308\n",
      "epochs =  0 , step =  2000 , current loss_val =  3.4776956545196698\n",
      "epochs =  0 , step =  3000 , current loss_val =  3.9398199253403914\n",
      "epochs =  0 , step =  4000 , current loss_val =  2.1267593419370154\n",
      "epochs =  0 , step =  5000 , current loss_val =  1.6443237450531472\n",
      "epochs =  0 , step =  6000 , current loss_val =  1.307701912962202\n",
      "epochs =  0 , step =  7000 , current loss_val =  2.795701718285013\n",
      "epochs =  0 , step =  8000 , current loss_val =  0.9526092258973469\n",
      "epochs =  0 , step =  9000 , current loss_val =  1.3943025700515792\n",
      "epochs =  0 , step =  10000 , current loss_val =  0.98194315334467\n",
      "epochs =  0 , step =  11000 , current loss_val =  1.0403011163726377\n",
      "epochs =  0 , step =  12000 , current loss_val =  1.0348025589462089\n",
      "epochs =  0 , step =  13000 , current loss_val =  1.1997847517563458\n",
      "epochs =  0 , step =  14000 , current loss_val =  0.6763529221425251\n",
      "epochs =  0 , step =  15000 , current loss_val =  1.0652480749283448\n",
      "epochs =  0 , step =  16000 , current loss_val =  0.831470931181096\n",
      "epochs =  0 , step =  17000 , current loss_val =  0.9121965320451975\n",
      "epochs =  0 , step =  18000 , current loss_val =  1.0099006804328428\n",
      "epochs =  0 , step =  19000 , current loss_val =  1.24590736235119\n",
      "epochs =  0 , step =  20000 , current loss_val =  0.7758299701574534\n",
      "epochs =  0 , step =  21000 , current loss_val =  1.0520890096663402\n",
      "epochs =  0 , step =  22000 , current loss_val =  0.8990535433849953\n",
      "epochs =  0 , step =  23000 , current loss_val =  0.7211189723478926\n",
      "epochs =  0 , step =  24000 , current loss_val =  0.8267529462526731\n",
      "epochs =  0 , step =  25000 , current loss_val =  1.2091483267147758\n",
      "epochs =  0 , step =  26000 , current loss_val =  0.7053255827536331\n",
      "epochs =  0 , step =  27000 , current loss_val =  1.0385166370263181\n",
      "epochs =  0 , step =  28000 , current loss_val =  0.6606398158706345\n",
      "epochs =  0 , step =  29000 , current loss_val =  0.7055185555852032\n",
      "epochs =  0 , step =  30000 , current loss_val =  0.6672438137918048\n",
      "epochs =  0 , step =  31000 , current loss_val =  1.36383905835019\n",
      "epochs =  0 , step =  32000 , current loss_val =  0.6281186393621901\n",
      "epochs =  0 , step =  33000 , current loss_val =  1.0184179624388912\n",
      "epochs =  0 , step =  34000 , current loss_val =  0.8855153445218032\n",
      "epochs =  0 , step =  35000 , current loss_val =  0.6402318060458232\n",
      "epochs =  0 , step =  36000 , current loss_val =  1.0074169590088835\n",
      "epochs =  0 , step =  37000 , current loss_val =  0.6606772712110387\n",
      "epochs =  0 , step =  38000 , current loss_val =  0.6590116771255806\n",
      "epochs =  0 , step =  39000 , current loss_val =  1.3684035503468643\n",
      "epochs =  0 , step =  40000 , current loss_val =  0.692212213274324\n",
      "epochs =  0 , step =  41000 , current loss_val =  0.6926840724032485\n",
      "epochs =  0 , step =  42000 , current loss_val =  0.6506407702949774\n",
      "epochs =  0 , step =  43000 , current loss_val =  1.0227630194644555\n",
      "epochs =  0 , step =  44000 , current loss_val =  0.7461125315818589\n",
      "epochs =  0 , step =  45000 , current loss_val =  0.7672854367567378\n",
      "epochs =  0 , step =  46000 , current loss_val =  0.7075085666620328\n",
      "epochs =  0 , step =  47000 , current loss_val =  0.764472686665991\n",
      "epochs =  0 , step =  48000 , current loss_val =  0.7028420285222748\n",
      "epochs =  0 , step =  49000 , current loss_val =  0.6688772167766271\n",
      "epochs =  0 , step =  50000 , current loss_val =  0.7898810553862633\n",
      "epochs =  0 , step =  51000 , current loss_val =  0.7433508498391284\n",
      "epochs =  0 , step =  52000 , current loss_val =  0.8296196406168961\n",
      "epochs =  0 , step =  53000 , current loss_val =  0.6606325353460951\n",
      "epochs =  0 , step =  54000 , current loss_val =  0.7740816902606215\n",
      "epochs =  0 , step =  55000 , current loss_val =  0.6753819917168177\n",
      "epochs =  0 , step =  56000 , current loss_val =  0.8092920355381944\n",
      "epochs =  0 , step =  57000 , current loss_val =  0.7315038673071736\n",
      "epochs =  0 , step =  58000 , current loss_val =  0.8730727844767661\n",
      "epochs =  0 , step =  59000 , current loss_val =  0.8556635104753472\n",
      "\n",
      "elapsed time =  0:00:35.161112\n"
     ]
    }
   ],
   "source": [
    "# hyper-parameter\n",
    "i_nodes = training_data.shape[1] - 1    # input nodes 개수\n",
    "h1_nodes = 30     # hidden 1 nodes\n",
    "h2_nodes = 20\n",
    "o_nodes = 10       # output nodes\n",
    "lr = 0.1           # learning rate\n",
    "epochs = 1         # epochs\n",
    "\n",
    "# 손실함수 값을 저장할 list 생성\n",
    "loss_val_list = []\n",
    "\n",
    "nn = NeuralNetwork(i_nodes, h1_nodes, h2_nodes, o_nodes, lr)\n",
    "\n",
    "start_time = datetime.now()\n",
    "\n",
    "for i in range(epochs):\n",
    "    \n",
    "    for step in range(len(training_data)):  # train\n",
    "    \n",
    "        # input_data, target_data normalize        \n",
    "        target_data = np.zeros(o_nodes) + 0.01    \n",
    "        target_data[int(training_data[step, 0])] = 0.99\n",
    "    \n",
    "        input_data = ((training_data[step, 1:] / 255.0) * 0.99) + 0.01\n",
    "    \n",
    "        nn.train( np.array(input_data, ndmin=2), np.array(target_data, ndmin=2) )  # train 할 경우에 행렬로 입력\n",
    "    \n",
    "        if step % 1000 == 0:\n",
    "            print(\"epochs = \", i, \", step = \", step,  \", current loss_val = \", nn.loss_val())\n",
    "        \n",
    "        # 손실함수 값 저장\n",
    "        loss_val_list.append(nn.loss_val())        \n",
    "        \n",
    "end_time = datetime.now() \n",
    "print(\"\\nelapsed time = \", end_time - start_time) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0~9 숫자 이미지가 784개의 숫자 (28X28) 로 구성되어 있는 test data 읽어옴\n",
    "\n",
    "try:\n",
    "    \n",
    "    test_data = np.loadtxt('./mnist_test.csv', delimiter=',', dtype=np.float32)\n",
    "\n",
    "    test_input_data = test_data[ : , 1: ]\n",
    "    test_target_data = test_data[ : , 0 ]\n",
    "\n",
    "    print(\"test_data.shape = \", test_data.shape)\n",
    "    print(\"test_data[0,0] = \", test_data[0,0], \", len(test_data[0]) = \", len(test_data[0]))\n",
    "\n",
    "    # measure accuracy\n",
    "    (accuracy_ret, false_list) = nn.accuracy(test_input_data, test_target_data)   \n",
    "\n",
    "    print('Accuracy = ', np.round(100*accuracy_ret, 3), ' %')\n",
    "    \n",
    "except Exception as err:\n",
    "    \n",
    "    print('Exception occur !!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 손실함수 추세 확인\n",
    "Y_DATA_LIST = []\n",
    "\n",
    "for index in range(0, len(loss_val_list), 500):\n",
    "    Y_DATA_LIST.append(loss_val_list[index])\n",
    "    \n",
    "plt.title('MNIST Loss Value Trend')\n",
    "plt.xlabel('data index ( X 500)')\n",
    "plt.ylabel('loss value')\n",
    "plt.grid()\n",
    "plt.plot(Y_DATA_LIST, color='b')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self.W4.shape =  (20, 10)\n",
      "self.A3,shape =  (1, 20)\n",
      "self.A3.T.shape =  (20, 1)\n",
      "loss_4.shape =  (1, 10)\n"
     ]
    }
   ],
   "source": [
    "n = NeuralNetwork(784, 30, 20, 10, 2)\n",
    "\n",
    "for step in range(len(training_data)):\n",
    "    target_data = np.zeros(10) + 0.01\n",
    "    target_data[int(training_data[step, 0])] = 0.99\n",
    "\n",
    "n.a(np.array(target_data, ndmin=2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
