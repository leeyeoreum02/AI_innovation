{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numerical_derivative(f, x):\n",
    "    delta_x = 1e-4 # 0.0001\n",
    "    grad = np.zeros_like(x)\n",
    "    \n",
    "    it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite'])\n",
    "    \n",
    "    while not it.finished:\n",
    "        idx = it.multi_index        \n",
    "        tmp_val = x[idx]\n",
    "        x[idx] = float(tmp_val) + delta_x\n",
    "        fx1 = f(x) # f(x+delta_x)\n",
    "        \n",
    "        x[idx] = float(tmp_val) - delta_x \n",
    "        fx2 = f(x) # f(x-delta_x)\n",
    "        grad[idx] = (fx1 - fx2) / (2*delta_x)\n",
    "        \n",
    "        x[idx] = tmp_val \n",
    "        it.iternext()   \n",
    "        \n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassificationTest:\n",
    "    def __init__(self, x_data, t_data, learning_rate, iteration_count):\n",
    "        \n",
    "        # 가중치 W 형상을 자동으로 구하기 위해 입력데이터가 vector 인지,\n",
    "        # 아니면 matrix 인지 체크 후, \n",
    "        # self.xdata 는 무조건 matrix 로 만들어 주면 코드 일관성이 있음\n",
    "        if x_data.ndim == 1:    # vector\n",
    "            self.x_data = x_data.reshape(len(x_data), 1)\n",
    "            self.t_data = t_data.reshape(len(t_data), 1)\n",
    "        elif x_data.ndim == 2:  # matrix\n",
    "            self.x_data = x_data\n",
    "            self.t_data = t_data\n",
    "        \n",
    "        self.learning_rate = learning_rate\n",
    "        self.iteration_count = iteration_count\n",
    "        \n",
    "        self.W = np.random.rand(self.x_data.shape[1], 1)\n",
    "        self.b = np.random.rand(1)\n",
    "        \n",
    "        self.loss_val_list = []\n",
    "        \n",
    "        print(\"ClassificationTest Object is created.\\n\")\n",
    "        \n",
    "    def get_W_b(self):\n",
    "        return self.W, self.b\n",
    "        \n",
    "    def __loss_func(self):\n",
    "        delta = 1e-7\n",
    "        \n",
    "        z = np.dot(self.x_data, self.W) + self.b\n",
    "        y = sigmoid(z)\n",
    "    \n",
    "        return -np.sum((self.t_data*np.log10(y + delta)) + ((1 - self.t_data)*np.log10((1 - y) + delta)))\n",
    "    \n",
    "    # 손실함수 값 계산 함수\n",
    "    # 입력변수 x, t : numpy type\n",
    "    def __loss_val(self):\n",
    "        delta = 1e-7\n",
    "        \n",
    "        z = np.dot(self.x_data, self.W) + self.b\n",
    "        y = sigmoid(z)\n",
    "    \n",
    "        return -np.sum((self.t_data*np.log10(y + delta)) + ((1 - self.t_data)*np.log10((1 - y) + delta)))\n",
    "\n",
    "    def train(self):\n",
    "        f = lambda x : self.__loss_func()\n",
    "        \n",
    "        print(\"Initial error value = \", self.__loss_val() , \"\\n\", \"Initial W = \", self.W, \"\\n\", \", b = \", self.b)\n",
    "\n",
    "        start_time = datetime.now()\n",
    "\n",
    "        for step in range(self.iteration_count):\n",
    "            self.W -= self.learning_rate * numerical_derivative(f, self.W)\n",
    "            self.b -= self.learning_rate * numerical_derivative(f, self.b)\n",
    "            \n",
    "            if step % (int)(0.05*self.iteration_count) == 0:\n",
    "                print(\"step = \", step, \"loss value = \", self.__loss_val())\n",
    "                self.loss_val_list.append(self.__loss_val())\n",
    "        \n",
    "        end_time = datetime.now()\n",
    "\n",
    "        print(\"\")\n",
    "        print(\"Elapsed Time => \", end_time - start_time)\n",
    "        \n",
    "    def display_lossval_trend(self):\n",
    "        plt.title('Loss Value Trend')\n",
    "        plt.xlabel('epochs ( X 1000)')\n",
    "        plt.ylabel('loss value')\n",
    "        plt.grid()\n",
    "\n",
    "        plt.plot(self.loss_val_list, ls='--', lw=2, label='lr={}, epoch={}'.format(self.learning_rate, self.iteration_count))\n",
    "        plt.legend(loc='best')\n",
    "\n",
    "        plt.show()\n",
    "    \n",
    "    # 학습을 마친 후, 임의의 데이터에 대해 미래 값 예측 함수\n",
    "    # 입력변수 x : numpy type\n",
    "    def predict(self, test_data):\n",
    "        z = np.dot(test_data, self.W) + self.b\n",
    "        y = sigmoid(z)\n",
    "        \n",
    "        if y >= 0.5:\n",
    "            result = 1\n",
    "        else:\n",
    "            result = 0\n",
    "    \n",
    "        return y, result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_data.ndim =  2 , x_data.shape =  (9, 2)\n",
      "t_data.ndim =  2 , t_data.shape =  (9, 1)\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    x_data = np.array([2, 4, 4, 11, 6, 6, 8, 5, 10, 7, 12, 16, 14, 8, 16, 3, 18, 7]).reshape(9, 2)\n",
    "    t_data = np.array([0, 0, 0, 0, 1, 1, 1, 1, 1]).reshape(9, 1)\n",
    "\n",
    "    print(\"x_data.ndim = \", x_data.ndim, \", x_data.shape = \", x_data.shape)\n",
    "    print(\"t_data.ndim = \", t_data.ndim, \", t_data.shape = \", t_data.shape) \n",
    "\n",
    "except FileNotFoundError as err:\n",
    "    print(str(err))\n",
    "except IndexError as err:\n",
    "    print(str(err))\n",
    "except Exception as err:\n",
    "    print(str(err))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ClassificationTest Object is created.\n",
      "\n",
      "Initial error value =  13.89220746760033 \n",
      " Initial W =  [[0.5488135 ]\n",
      " [0.71518937]] \n",
      " , b =  [0.60276338]\n",
      "step =  0 loss value =  15.314584792668047\n",
      "step =  20 loss value =  6.9878078506969645\n",
      "step =  40 loss value =  4.9857036887036505\n",
      "step =  60 loss value =  6.085276965614443\n",
      "step =  80 loss value =  4.494624168141165\n",
      "step =  100 loss value =  5.130741491611701\n",
      "step =  120 loss value =  4.3160289226220945\n",
      "step =  140 loss value =  4.005363134126211\n",
      "step =  160 loss value =  3.7533159714846134\n",
      "step =  180 loss value =  3.481776261236963\n",
      "step =  200 loss value =  3.223840650468413\n",
      "step =  220 loss value =  2.977577700752557\n",
      "step =  240 loss value =  2.741414502061187\n",
      "step =  260 loss value =  2.5144630541498993\n",
      "step =  280 loss value =  2.296537391132243\n",
      "step =  300 loss value =  2.0881209743257534\n",
      "step =  320 loss value =  1.8902280694479183\n",
      "step =  340 loss value =  1.7041275809253023\n",
      "step =  360 loss value =  1.530914240852937\n",
      "step =  380 loss value =  1.3707335619760899\n",
      "step =  400 loss value =  1.2195681374450698\n",
      "step =  420 loss value =  0.9713923510422773\n",
      "step =  440 loss value =  0.20118562580418417\n",
      "step =  460 loss value =  0.19660814073523616\n",
      "step =  480 loss value =  0.19395561367984196\n",
      "step =  500 loss value =  0.1920888363840102\n",
      "step =  520 loss value =  0.1906137429024747\n",
      "step =  540 loss value =  0.1893281067492692\n",
      "step =  560 loss value =  0.1881331009987294\n",
      "step =  580 loss value =  0.1869839428571254\n",
      "step =  600 loss value =  0.18586154177420836\n",
      "step =  620 loss value =  0.1847580247387781\n",
      "step =  640 loss value =  0.18367013234870408\n",
      "step =  660 loss value =  0.18259643483854587\n",
      "step =  680 loss value =  0.18153621540633924\n",
      "step =  700 loss value =  0.18048903507289704\n",
      "step =  720 loss value =  0.17945456564363033\n",
      "step =  740 loss value =  0.1784325259384037\n",
      "step =  760 loss value =  0.17742265738835827\n",
      "step =  780 loss value =  0.17642471458270678\n",
      "step =  800 loss value =  0.17543846149458167\n",
      "step =  820 loss value =  0.1744636698715893\n",
      "step =  840 loss value =  0.17350011845748164\n",
      "step =  860 loss value =  0.17254759253770702\n",
      "step =  880 loss value =  0.17160588361557694\n",
      "step =  900 loss value =  0.1706747891448614\n",
      "step =  920 loss value =  0.16975411229038848\n",
      "step =  940 loss value =  0.16884366170552234\n",
      "step =  960 loss value =  0.16794325132222448\n",
      "step =  980 loss value =  0.16705270015144622\n",
      "step =  1000 loss value =  0.16617183209309572\n",
      "step =  1020 loss value =  0.16530047575483126\n",
      "step =  1040 loss value =  0.16443846427918918\n",
      "step =  1060 loss value =  0.16358563517863756\n",
      "step =  1080 loss value =  0.1627418301780984\n",
      "step =  1100 loss value =  0.16190689506461065\n",
      "step =  1120 loss value =  0.1610806795436993\n",
      "step =  1140 loss value =  0.16026303710214412\n",
      "step =  1160 loss value =  0.1594538248768668\n",
      "step =  1180 loss value =  0.158652903529509\n",
      "step =  1200 loss value =  0.1578601371264132\n",
      "step =  1220 loss value =  0.1570753930239569\n",
      "step =  1240 loss value =  0.15629854175861063\n",
      "step =  1260 loss value =  0.15552945694178624\n",
      "step =  1280 loss value =  0.15476801515908417\n",
      "step =  1300 loss value =  0.1540140958736826\n",
      "step =  1320 loss value =  0.15326758133388352\n",
      "step =  1340 loss value =  0.15252835648431615\n",
      "step =  1360 loss value =  0.1517963088808221\n",
      "step =  1380 loss value =  0.15107132860879047\n",
      "step =  1400 loss value =  0.15035330820477122\n",
      "step =  1420 loss value =  0.14964214258120148\n",
      "step =  1440 loss value =  0.14893772895407478\n",
      "step =  1460 loss value =  0.14823996677346613\n",
      "step =  1480 loss value =  0.14754875765682823\n",
      "step =  1500 loss value =  0.14686400532468688\n",
      "step =  1520 loss value =  0.14618561553899515\n",
      "step =  1540 loss value =  0.14551349604364897\n",
      "step =  1560 loss value =  0.1448475565073474\n",
      "step =  1580 loss value =  0.1441877084685439\n",
      "step =  1600 loss value =  0.14353386528247097\n",
      "step =  1620 loss value =  0.14288594207001296\n",
      "step =  1640 loss value =  0.14224385566857706\n",
      "step =  1660 loss value =  0.14160752458458967\n",
      "step =  1680 loss value =  0.14097686894783248\n",
      "step =  1700 loss value =  0.14035181046730968\n",
      "step =  1720 loss value =  0.1397322723886845\n",
      "step =  1740 loss value =  0.13911817945322563\n",
      "step =  1760 loss value =  0.13850945785818577\n",
      "step =  1780 loss value =  0.13790603521846254\n",
      "step =  1800 loss value =  0.1373078405296853\n",
      "step =  1820 loss value =  0.1367148041324769\n",
      "step =  1840 loss value =  0.1361268576779133\n",
      "step =  1860 loss value =  0.13554393409415055\n",
      "step =  1880 loss value =  0.13496596755418397\n",
      "step =  1900 loss value =  0.1343928934446074\n",
      "step =  1920 loss value =  0.13382464833540877\n",
      "step =  1940 loss value =  0.1332611699507746\n",
      "step =  1960 loss value =  0.13270239714076548\n",
      "step =  1980 loss value =  0.1321482698539754\n",
      "step =  2000 loss value =  0.1315987291109672\n",
      "step =  2020 loss value =  0.13105371697858384\n",
      "step =  2040 loss value =  0.13051317654506814\n",
      "step =  2060 loss value =  0.12997705189590653\n",
      "step =  2080 loss value =  0.12944528809045186\n",
      "step =  2100 loss value =  0.1289178311392406\n",
      "step =  2120 loss value =  0.1283946279819824\n",
      "step =  2140 loss value =  0.12787562646622797\n",
      "step =  2160 loss value =  0.12736077532665405\n",
      "step =  2180 loss value =  0.1268500241649679\n",
      "step =  2200 loss value =  0.12634332343040908\n",
      "step =  2220 loss value =  0.12584062440077676\n",
      "step =  2240 loss value =  0.12534187916404757\n",
      "step =  2260 loss value =  0.12484704060051988\n",
      "step =  2280 loss value =  0.12435606236542086\n",
      "step =  2300 loss value =  0.12386889887203124\n",
      "step =  2320 loss value =  0.1233855052753185\n",
      "step =  2340 loss value =  0.12290583745595726\n",
      "step =  2360 loss value =  0.12242985200484147\n",
      "step =  2380 loss value =  0.12195750620799427\n",
      "step =  2400 loss value =  0.12148875803190565\n",
      "step =  2420 loss value =  0.12102356610926687\n",
      "step =  2440 loss value =  0.12056188972506672\n",
      "step =  2460 loss value =  0.12010368880307762\n",
      "step =  2480 loss value =  0.11964892389270936\n",
      "step =  2500 loss value =  0.11919755615619596\n",
      "step =  2520 loss value =  0.11874954735610728\n",
      "step =  2540 loss value =  0.11830485984318263\n",
      "step =  2560 loss value =  0.11786345654452444\n",
      "step =  2580 loss value =  0.11742530095205207\n",
      "step =  2600 loss value =  0.11699035711125494\n",
      "step =  2620 loss value =  0.11655858961023405\n",
      "step =  2640 loss value =  0.11612996356902809\n",
      "step =  2660 loss value =  0.11570444462921158\n",
      "step =  2680 loss value =  0.11528199894372017\n",
      "step =  2700 loss value =  0.11486259316696189\n",
      "step =  2720 loss value =  0.11444619444515408\n",
      "step =  2740 loss value =  0.11403277040689341\n",
      "step =  2760 loss value =  0.11362228915397883\n",
      "step =  2780 loss value =  0.11321471925242632\n",
      "step =  2800 loss value =  0.11281002972371279\n",
      "step =  2820 loss value =  0.11240819003622554\n",
      "step =  2840 loss value =  0.11200917009691984\n",
      "step =  2860 loss value =  0.1116129402431742\n",
      "step =  2880 loss value =  0.11121947123481904\n",
      "step =  2900 loss value =  0.11082873424637686\n",
      "step =  2920 loss value =  0.11044070085946998\n",
      "step =  2940 loss value =  0.11005534305539749\n",
      "step =  2960 loss value =  0.10967263320788807\n",
      "step =  2980 loss value =  0.10929254407600596\n",
      "step =  3000 loss value =  0.10891504879726213\n",
      "step =  3020 loss value =  0.10854012088082564\n",
      "step =  3040 loss value =  0.10816773420091236\n",
      "step =  3060 loss value =  0.10779786299030861\n",
      "step =  3080 loss value =  0.10743048183408407\n",
      "step =  3100 loss value =  0.10706556566335323\n",
      "step =  3120 loss value =  0.10670308974929395\n",
      "step =  3140 loss value =  0.10634302969717445\n",
      "step =  3160 loss value =  0.10598536144060236\n",
      "step =  3180 loss value =  0.10563006123585035\n",
      "step =  3200 loss value =  0.10527710565631919\n",
      "step =  3220 loss value =  0.1049264715871073\n",
      "step =  3240 loss value =  0.10457813621973928\n",
      "step =  3260 loss value =  0.10423207704690512\n",
      "step =  3280 loss value =  0.10388827185746882\n",
      "step =  3300 loss value =  0.1035466987314157\n",
      "step =  3320 loss value =  0.10320733603501085\n",
      "step =  3340 loss value =  0.1028701624160308\n",
      "step =  3360 loss value =  0.10253515679908826\n",
      "step =  3380 loss value =  0.10220229838103666\n",
      "step =  3400 loss value =  0.10187156662652251\n",
      "step =  3420 loss value =  0.1015429412635707\n",
      "step =  3440 loss value =  0.10121640227929188\n",
      "step =  3460 loss value =  0.1008919299156609\n",
      "step =  3480 loss value =  0.10056950466539694\n",
      "step =  3500 loss value =  0.10024910726791252\n",
      "step =  3520 loss value =  0.0999307187053409\n",
      "step =  3540 loss value =  0.09961432019866301\n",
      "step =  3560 loss value =  0.09929989320388367\n",
      "step =  3580 loss value =  0.0989874194083069\n",
      "step =  3600 loss value =  0.09867688072686842\n",
      "step =  3620 loss value =  0.09836825929854744\n",
      "step =  3640 loss value =  0.09806153748283754\n",
      "step =  3660 loss value =  0.09775669785631154\n",
      "step =  3680 loss value =  0.09745372320922503\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step =  3700 loss value =  0.09715259654220014\n",
      "step =  3720 loss value =  0.09685330106295587\n",
      "step =  3740 loss value =  0.09655582018313388\n",
      "step =  3760 loss value =  0.09626013751514072\n",
      "step =  3780 loss value =  0.0959662368690933\n",
      "step =  3800 loss value =  0.09567410224979069\n",
      "step =  3820 loss value =  0.09538371785375857\n",
      "step =  3840 loss value =  0.09509506806634357\n",
      "step =  3860 loss value =  0.0948081374588396\n",
      "step =  3880 loss value =  0.09452291078572948\n",
      "step =  3900 loss value =  0.09423937298191282\n",
      "step =  3920 loss value =  0.09395750916000575\n",
      "step =  3940 loss value =  0.0936773046077067\n",
      "step =  3960 loss value =  0.0933987447851959\n",
      "step =  3980 loss value =  0.09312181532257127\n",
      "step =  4000 loss value =  0.09284650201735513\n",
      "step =  4020 loss value =  0.09257279083204718\n",
      "step =  4040 loss value =  0.09230066789166964\n",
      "step =  4060 loss value =  0.09203011948142577\n",
      "step =  4080 loss value =  0.09176113204436483\n",
      "step =  4100 loss value =  0.09149369217908851\n",
      "step =  4120 loss value =  0.09122778663749664\n",
      "step =  4140 loss value =  0.09096340232258524\n",
      "step =  4160 loss value =  0.09070052628628712\n",
      "step =  4180 loss value =  0.09043914572731131\n",
      "step =  4200 loss value =  0.09017924798908984\n",
      "step =  4220 loss value =  0.0899208205576781\n",
      "step =  4240 loss value =  0.0896638510597471\n",
      "step =  4260 loss value =  0.08940832726061426\n",
      "step =  4280 loss value =  0.08915423706226187\n",
      "step =  4300 loss value =  0.08890156850144296\n",
      "step =  4320 loss value =  0.08865030974776479\n",
      "step =  4340 loss value =  0.0884004491018588\n",
      "step =  4360 loss value =  0.0881519749935503\n",
      "step =  4380 loss value =  0.08790487598006305\n",
      "step =  4400 loss value =  0.08765914074425894\n",
      "step =  4420 loss value =  0.0874147580928963\n",
      "step =  4440 loss value =  0.08717171695494981\n",
      "step =  4460 loss value =  0.08693000637991154\n",
      "step =  4480 loss value =  0.0866896155361567\n",
      "step =  4500 loss value =  0.08645053370931925\n",
      "step =  4520 loss value =  0.08621275030069098\n",
      "step =  4540 loss value =  0.08597625482567875\n",
      "step =  4560 loss value =  0.08574103691223393\n",
      "step =  4580 loss value =  0.08550708629935182\n",
      "step =  4600 loss value =  0.08527439283558579\n",
      "step =  4620 loss value =  0.08504294647757288\n",
      "step =  4640 loss value =  0.08481273728859191\n",
      "step =  4660 loss value =  0.0845837554371403\n",
      "step =  4680 loss value =  0.0843559911955418\n",
      "step =  4700 loss value =  0.0841294349385737\n",
      "step =  4720 loss value =  0.08390407714211254\n",
      "step =  4740 loss value =  0.08367990838180495\n",
      "step =  4760 loss value =  0.08345691933174987\n",
      "step =  4780 loss value =  0.08323510076323003\n",
      "step =  4800 loss value =  0.08301444354341966\n",
      "step =  4820 loss value =  0.08279493863415037\n",
      "step =  4840 loss value =  0.08257657709067723\n",
      "step =  4860 loss value =  0.0823593500604851\n",
      "step =  4880 loss value =  0.08214324878206916\n",
      "step =  4900 loss value =  0.08192826458378706\n",
      "step =  4920 loss value =  0.08171438888269972\n",
      "step =  4940 loss value =  0.08150161318343027\n",
      "step =  4960 loss value =  0.08128992907704954\n",
      "step =  4980 loss value =  0.08107932823998282\n",
      "step =  5000 loss value =  0.08086980243290548\n",
      "step =  5020 loss value =  0.0806613434997075\n",
      "step =  5040 loss value =  0.08045394336641226\n",
      "step =  5060 loss value =  0.08024759404015283\n",
      "step =  5080 loss value =  0.08004228760816481\n",
      "step =  5100 loss value =  0.0798380162367621\n",
      "step =  5120 loss value =  0.07963477217036413\n",
      "step =  5140 loss value =  0.07943254773051331\n",
      "step =  5160 loss value =  0.07923133531491942\n",
      "step =  5180 loss value =  0.07903112739651591\n",
      "step =  5200 loss value =  0.07883191652253656\n",
      "step =  5220 loss value =  0.07863369531357786\n",
      "step =  5240 loss value =  0.07843645646272049\n",
      "step =  5260 loss value =  0.0782401927346218\n",
      "step =  5280 loss value =  0.07804489696465045\n",
      "step =  5300 loss value =  0.07785056205801959\n",
      "step =  5320 loss value =  0.07765718098893869\n",
      "step =  5340 loss value =  0.0774647467997658\n",
      "step =  5360 loss value =  0.07727325260019258\n",
      "step =  5380 loss value =  0.07708269156642514\n",
      "step =  5400 loss value =  0.07689305694038924\n",
      "step =  5420 loss value =  0.07670434202892275\n",
      "step =  5440 loss value =  0.07651654020302197\n",
      "step =  5460 loss value =  0.0763296448970459\n",
      "step =  5480 loss value =  0.07614364960798711\n",
      "step =  5500 loss value =  0.07595854789470392\n",
      "step =  5520 loss value =  0.07577433337720163\n",
      "step =  5540 loss value =  0.07559099973589374\n",
      "step =  5560 loss value =  0.07540854071090283\n",
      "step =  5580 loss value =  0.07522695010134166\n",
      "step =  5600 loss value =  0.07504622176462031\n",
      "step =  5620 loss value =  0.07486634961579443\n",
      "step =  5640 loss value =  0.07468732762683217\n",
      "step =  5660 loss value =  0.07450914982599831\n",
      "step =  5680 loss value =  0.07433181029717441\n",
      "step =  5700 loss value =  0.07415530317922704\n",
      "step =  5720 loss value =  0.0739796226653595\n",
      "step =  5740 loss value =  0.07380476300248763\n",
      "step =  5760 loss value =  0.0736307184906126\n",
      "step =  5780 loss value =  0.07345748348222457\n",
      "step =  5800 loss value =  0.07328505238168488\n",
      "step =  5820 loss value =  0.07311341964463766\n",
      "step =  5840 loss value =  0.07294257977742905\n",
      "step =  5860 loss value =  0.0727725273365263\n",
      "step =  5880 loss value =  0.07260325692793328\n",
      "step =  5900 loss value =  0.07243476320665093\n",
      "step =  5920 loss value =  0.07226704087610344\n",
      "step =  5940 loss value =  0.07210008468760608\n",
      "step =  5960 loss value =  0.07193388943981062\n",
      "step =  5980 loss value =  0.07176844997819481\n",
      "step =  6000 loss value =  0.07160376119449895\n",
      "step =  6020 loss value =  0.07143981802625124\n",
      "step =  6040 loss value =  0.07127661545622987\n",
      "step =  6060 loss value =  0.07111414851197168\n",
      "step =  6080 loss value =  0.07095241226525976\n",
      "step =  6100 loss value =  0.07079140183164136\n",
      "step =  6120 loss value =  0.07063111236994639\n",
      "step =  6140 loss value =  0.07047153908180853\n",
      "step =  6160 loss value =  0.07031267721118611\n",
      "step =  6180 loss value =  0.07015452204390674\n",
      "step =  6200 loss value =  0.06999706890719896\n",
      "step =  6220 loss value =  0.06984031316923828\n",
      "step =  6240 loss value =  0.06968425023871844\n",
      "step =  6260 loss value =  0.06952887556438157\n",
      "step =  6280 loss value =  0.06937418463461027\n",
      "step =  6300 loss value =  0.06922017297697555\n",
      "step =  6320 loss value =  0.06906683615782246\n",
      "step =  6340 loss value =  0.06891416978185229\n",
      "step =  6360 loss value =  0.06876216949170175\n",
      "step =  6380 loss value =  0.06861083096755208\n",
      "step =  6400 loss value =  0.06846014992670259\n",
      "step =  6420 loss value =  0.06831012212318809\n",
      "step =  6440 loss value =  0.06816074334738234\n",
      "step =  6460 loss value =  0.06801200942561593\n",
      "step =  6480 loss value =  0.06786391621978782\n",
      "step =  6500 loss value =  0.06771645962698221\n",
      "step =  6520 loss value =  0.06756963557910768\n",
      "step =  6540 loss value =  0.06742344004252308\n",
      "step =  6560 loss value =  0.06727786901767407\n",
      "step =  6580 loss value =  0.06713291853873997\n",
      "step =  6600 loss value =  0.06698858467326667\n",
      "step =  6620 loss value =  0.06684486352183204\n",
      "step =  6640 loss value =  0.06670175121768869\n",
      "step =  6660 loss value =  0.06655924392642296\n",
      "step =  6680 loss value =  0.06641733784563805\n",
      "step =  6700 loss value =  0.06627602920458084\n",
      "step =  6720 loss value =  0.06613531426386271\n",
      "step =  6740 loss value =  0.06599518931509342\n",
      "step =  6760 loss value =  0.06585565068059185\n",
      "step =  6780 loss value =  0.06571669471304507\n",
      "step =  6800 loss value =  0.06557831779521196\n",
      "step =  6820 loss value =  0.06544051633960517\n",
      "step =  6840 loss value =  0.06530328678819143\n",
      "step =  6860 loss value =  0.06516662561208932\n",
      "step =  6880 loss value =  0.06503052931127588\n",
      "step =  6900 loss value =  0.06489499441427907\n",
      "step =  6920 loss value =  0.0647600174779018\n",
      "step =  6940 loss value =  0.0646255950869372\n",
      "step =  6960 loss value =  0.06449172385387189\n",
      "step =  6980 loss value =  0.06435840041861558\n",
      "step =  7000 loss value =  0.06422562144821735\n",
      "step =  7020 loss value =  0.06409338363660462\n",
      "step =  7040 loss value =  0.06396168370429985\n",
      "step =  7060 loss value =  0.06383051839816199\n",
      "step =  7080 loss value =  0.06369988449112406\n",
      "step =  7100 loss value =  0.06356977878192518\n",
      "step =  7120 loss value =  0.06344019809487055\n",
      "step =  7140 loss value =  0.0633111392795572\n",
      "step =  7160 loss value =  0.06318259921063188\n",
      "step =  7180 loss value =  0.06305457478754514\n",
      "step =  7200 loss value =  0.06292706293430343\n",
      "step =  7220 loss value =  0.06280006059922379\n",
      "step =  7240 loss value =  0.06267356475470416\n",
      "step =  7260 loss value =  0.06254757239697974\n",
      "step =  7280 loss value =  0.06242208054588405\n",
      "step =  7300 loss value =  0.06229708624463236\n",
      "step =  7320 loss value =  0.062172586559588565\n",
      "step =  7340 loss value =  0.0620485785800311\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step =  7360 loss value =  0.06192505941793461\n",
      "step =  7380 loss value =  0.06180202620775614\n",
      "step =  7400 loss value =  0.06167947610620985\n",
      "step =  7420 loss value =  0.0615574062920558\n",
      "step =  7440 loss value =  0.061435813965876926\n",
      "step =  7460 loss value =  0.061314696349886844\n",
      "step =  7480 loss value =  0.06119405068771265\n",
      "step =  7500 loss value =  0.06107387424418161\n",
      "step =  7520 loss value =  0.06095416430513117\n",
      "step =  7540 loss value =  0.060834918177202185\n",
      "step =  7560 loss value =  0.06071613318763151\n",
      "step =  7580 loss value =  0.060597806684081414\n",
      "step =  7600 loss value =  0.060479936034419556\n",
      "step =  7620 loss value =  0.06036251862654239\n",
      "step =  7640 loss value =  0.0602455518681742\n",
      "step =  7660 loss value =  0.06012903318669056\n",
      "step =  7680 loss value =  0.06001296002892461\n",
      "step =  7700 loss value =  0.059897329860984824\n",
      "step =  7720 loss value =  0.059782140168083614\n",
      "step =  7740 loss value =  0.05966738845434774\n",
      "step =  7760 loss value =  0.05955307224264688\n",
      "step =  7780 loss value =  0.059439189074410745\n",
      "step =  7800 loss value =  0.059325736509465925\n",
      "step =  7820 loss value =  0.05921271212585633\n",
      "step =  7840 loss value =  0.05910011351967647\n",
      "step =  7860 loss value =  0.05898793830491154\n",
      "step =  7880 loss value =  0.0588761841132649\n",
      "step =  7900 loss value =  0.058764848593985265\n",
      "step =  7920 loss value =  0.058653929413722494\n",
      "step =  7940 loss value =  0.05854342425635574\n",
      "step =  7960 loss value =  0.05843333082283737\n",
      "step =  7980 loss value =  0.05832364683104437\n",
      "step =  8000 loss value =  0.0582143700156098\n",
      "step =  8020 loss value =  0.058105498127778404\n",
      "step =  8040 loss value =  0.057997028935250286\n",
      "step =  8060 loss value =  0.05788896022203731\n",
      "step =  8080 loss value =  0.05778128978831067\n",
      "step =  8100 loss value =  0.05767401545025533\n",
      "step =  8120 loss value =  0.05756713503992166\n",
      "step =  8140 loss value =  0.05746064640508336\n",
      "step =  8160 loss value =  0.05735454740909826\n",
      "step =  8180 loss value =  0.05724883593077191\n",
      "step =  8200 loss value =  0.057143509864208734\n",
      "step =  8220 loss value =  0.0570385671186724\n",
      "step =  8240 loss value =  0.0569340056184694\n",
      "step =  8260 loss value =  0.056829823302794916\n",
      "step =  8280 loss value =  0.05672601812560788\n",
      "step =  8300 loss value =  0.056622588055494937\n",
      "step =  8320 loss value =  0.05651953107555265\n",
      "step =  8340 loss value =  0.056416845183239465\n",
      "step =  8360 loss value =  0.056314528390261465\n",
      "step =  8380 loss value =  0.056212578722445616\n",
      "step =  8400 loss value =  0.056110994219611667\n",
      "step =  8420 loss value =  0.05600977293544684\n",
      "step =  8440 loss value =  0.05590891293738781\n",
      "step =  8460 loss value =  0.05580841230649705\n",
      "step =  8480 loss value =  0.055708269137349976\n",
      "step =  8500 loss value =  0.05560848153791026\n",
      "step =  8520 loss value =  0.05550904762941435\n",
      "step =  8540 loss value =  0.05540996554625189\n",
      "step =  8560 loss value =  0.055311233435852716\n",
      "step =  8580 loss value =  0.055212849458578704\n",
      "step =  8600 loss value =  0.055114811787604225\n",
      "step =  8620 loss value =  0.05501711860881402\n",
      "step =  8640 loss value =  0.054919768120683055\n",
      "step =  8660 loss value =  0.05482275853417732\n",
      "step =  8680 loss value =  0.054726088072635785\n",
      "step =  8700 loss value =  0.05462975497167146\n",
      "step =  8720 loss value =  0.05453375747906431\n",
      "step =  8740 loss value =  0.05443809385465895\n",
      "step =  8760 loss value =  0.05434276237025661\n",
      "step =  8780 loss value =  0.05424776130951443\n",
      "step =  8800 loss value =  0.05415308896784112\n",
      "step =  8820 loss value =  0.05405874365231059\n",
      "step =  8840 loss value =  0.05396472368154164\n",
      "step =  8860 loss value =  0.05387102738561859\n",
      "step =  8880 loss value =  0.05377765310598426\n",
      "step =  8900 loss value =  0.053684599195342894\n",
      "step =  8920 loss value =  0.053591864017569914\n",
      "step =  8940 loss value =  0.05349944594761793\n",
      "step =  8960 loss value =  0.05340734337141742\n",
      "step =  8980 loss value =  0.05331555468579353\n",
      "step =  9000 loss value =  0.053224078298365785\n",
      "step =  9020 loss value =  0.05313291262745987\n",
      "step =  9040 loss value =  0.053042056102023755\n",
      "step =  9060 loss value =  0.05295150716152545\n",
      "step =  9080 loss value =  0.05286126425589191\n",
      "step =  9100 loss value =  0.05277132584538144\n",
      "step =  9120 loss value =  0.052681690400539154\n",
      "step =  9140 loss value =  0.052592356402083276\n",
      "step =  9160 loss value =  0.052503322340835934\n",
      "step =  9180 loss value =  0.05241458671763454\n",
      "step =  9200 loss value =  0.052326148043245126\n",
      "step =  9220 loss value =  0.05223800483828581\n",
      "step =  9240 loss value =  0.05215015563315127\n",
      "step =  9260 loss value =  0.052062598967910004\n",
      "step =  9280 loss value =  0.05197533339225704\n",
      "step =  9300 loss value =  0.05188835746540776\n",
      "step =  9320 loss value =  0.051801669756036915\n",
      "step =  9340 loss value =  0.05171526884218763\n",
      "step =  9360 loss value =  0.05162915331121303\n",
      "step =  9380 loss value =  0.05154332175968174\n",
      "step =  9400 loss value =  0.05145777279331624\n",
      "step =  9420 loss value =  0.05137250502690999\n",
      "step =  9440 loss value =  0.051287517084256266\n",
      "step =  9460 loss value =  0.051202807598082656\n",
      "step =  9480 loss value =  0.051118375209972665\n",
      "step =  9500 loss value =  0.05103421857029014\n",
      "step =  9520 loss value =  0.05095033633811358\n",
      "step =  9540 loss value =  0.05086672718117066\n",
      "step =  9560 loss value =  0.0507833897757583\n",
      "step =  9580 loss value =  0.05070032280668589\n",
      "step =  9600 loss value =  0.05061752496719278\n",
      "step =  9620 loss value =  0.050534994958894075\n",
      "step =  9640 loss value =  0.050452731491712303\n",
      "step =  9660 loss value =  0.05037073328380467\n",
      "step =  9680 loss value =  0.05028899906149871\n",
      "step =  9700 loss value =  0.05020752755923184\n",
      "step =  9720 loss value =  0.05012631751948208\n",
      "step =  9740 loss value =  0.050045367692717495\n",
      "step =  9760 loss value =  0.04996467683730658\n",
      "step =  9780 loss value =  0.04988424371948976\n",
      "step =  9800 loss value =  0.049804067113287394\n",
      "step =  9820 loss value =  0.04972414580045778\n",
      "step =  9840 loss value =  0.04964447857042378\n",
      "step =  9860 loss value =  0.04956506422022582\n",
      "step =  9880 loss value =  0.04948590155445595\n",
      "step =  9900 loss value =  0.049406989385187526\n",
      "step =  9920 loss value =  0.049328326531939325\n",
      "step =  9940 loss value =  0.04924991182160014\n",
      "step =  9960 loss value =  0.04917174408838008\n",
      "step =  9980 loss value =  0.04909382217374545\n",
      "step =  10000 loss value =  0.04901614492637792\n",
      "step =  10020 loss value =  0.048938711202098306\n",
      "step =  10040 loss value =  0.04886151986382476\n",
      "step =  10060 loss value =  0.04878456978151863\n",
      "step =  10080 loss value =  0.04870785983212266\n",
      "step =  10100 loss value =  0.04863138889951161\n",
      "step =  10120 loss value =  0.048555155874442105\n",
      "step =  10140 loss value =  0.04847915965449412\n",
      "step =  10160 loss value =  0.048403399144017104\n",
      "step =  10180 loss value =  0.048327873254086925\n",
      "step =  10200 loss value =  0.04825258090245256\n",
      "step =  10220 loss value =  0.04817752101347208\n",
      "step =  10240 loss value =  0.04810269251808652\n",
      "step =  10260 loss value =  0.048028094353741876\n",
      "step =  10280 loss value =  0.0479537254643597\n",
      "step =  10300 loss value =  0.04787958480028703\n",
      "step =  10320 loss value =  0.04780567131823514\n",
      "step =  10340 loss value =  0.04773198398124085\n",
      "step =  10360 loss value =  0.04765852175862448\n",
      "step =  10380 loss value =  0.04758528362592615\n",
      "step =  10400 loss value =  0.04751226856487112\n",
      "step =  10420 loss value =  0.04743947556332223\n",
      "step =  10440 loss value =  0.04736690361522841\n",
      "step =  10460 loss value =  0.04729455172058491\n",
      "step =  10480 loss value =  0.04722241888538659\n",
      "step =  10500 loss value =  0.04715050412158148\n",
      "step =  10520 loss value =  0.04707880644702595\n",
      "step =  10540 loss value =  0.047007324885444915\n",
      "step =  10560 loss value =  0.04693605846637947\n",
      "step =  10580 loss value =  0.04686500622515534\n",
      "step =  10600 loss value =  0.04679416720283626\n",
      "step =  10620 loss value =  0.04672354044617246\n",
      "step =  10640 loss value =  0.04665312500757062\n",
      "step =  10660 loss value =  0.046582919945043766\n",
      "step =  10680 loss value =  0.04651292432217444\n",
      "step =  10700 loss value =  0.04644313720807559\n",
      "step =  10720 loss value =  0.046373557677348375\n",
      "step =  10740 loss value =  0.04630418481003327\n",
      "step =  10760 loss value =  0.046235017691581504\n",
      "step =  10780 loss value =  0.04616605541281152\n",
      "step =  10800 loss value =  0.046097297069872825\n",
      "step =  10820 loss value =  0.04602874176419997\n",
      "step =  10840 loss value =  0.04596038860247842\n",
      "step =  10860 loss value =  0.04589223669660805\n",
      "step =  10880 loss value =  0.0458242851636606\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step =  10900 loss value =  0.045756533125849576\n",
      "step =  10920 loss value =  0.04568897971048288\n",
      "step =  10940 loss value =  0.04562162404993693\n",
      "step =  10960 loss value =  0.04555446528160865\n",
      "step =  10980 loss value =  0.04548750254788787\n",
      "step =  11000 loss value =  0.04542073499612125\n",
      "step =  11020 loss value =  0.04535416177857258\n",
      "step =  11040 loss value =  0.045287782052384616\n",
      "step =  11060 loss value =  0.04522159497955725\n",
      "step =  11080 loss value =  0.04515559972689602\n",
      "step =  11100 loss value =  0.0450897954659863\n",
      "step =  11120 loss value =  0.04502418137316348\n",
      "step =  11140 loss value =  0.04495875662947165\n",
      "step =  11160 loss value =  0.04489352042062814\n",
      "step =  11180 loss value =  0.04482847193700282\n",
      "step =  11200 loss value =  0.04476361037357405\n",
      "step =  11220 loss value =  0.04469893492989866\n",
      "step =  11240 loss value =  0.0446344448100737\n",
      "step =  11260 loss value =  0.04457013922272023\n",
      "step =  11280 loss value =  0.044506017380940226\n",
      "step =  11300 loss value =  0.044442078502278395\n",
      "step =  11320 loss value =  0.04437832180870944\n",
      "step =  11340 loss value =  0.04431474652658608\n",
      "step =  11360 loss value =  0.04425135188662926\n",
      "step =  11380 loss value =  0.04418813712387839\n",
      "step =  11400 loss value =  0.044125101477674275\n",
      "step =  11420 loss value =  0.0440622441916217\n",
      "step =  11440 loss value =  0.04399956451356446\n",
      "step =  11460 loss value =  0.04393706169555195\n",
      "step =  11480 loss value =  0.04387473499381048\n",
      "step =  11500 loss value =  0.043812583668723315\n",
      "step =  11520 loss value =  0.04375060698478183\n",
      "step =  11540 loss value =  0.043688804210578\n",
      "step =  11560 loss value =  0.04362717461876474\n",
      "step =  11580 loss value =  0.04356571748602719\n",
      "step =  11600 loss value =  0.0435044320930608\n",
      "step =  11620 loss value =  0.04344331772454474\n",
      "step =  11640 loss value =  0.04338237366910203\n",
      "step =  11660 loss value =  0.04332159921928668\n",
      "step =  11680 loss value =  0.04326099367154476\n",
      "step =  11700 loss value =  0.043200556326204155\n",
      "step =  11720 loss value =  0.043140286487424906\n",
      "step =  11740 loss value =  0.04308018346319708\n",
      "step =  11760 loss value =  0.04302024656529686\n",
      "step =  11780 loss value =  0.042960475109270445\n",
      "step =  11800 loss value =  0.04290086841440144\n",
      "step =  11820 loss value =  0.042841425803691546\n",
      "step =  11840 loss value =  0.04278214660382956\n",
      "step =  11860 loss value =  0.042723030145174846\n",
      "step =  11880 loss value =  0.04266407576172238\n",
      "step =  11900 loss value =  0.04260528279108456\n",
      "step =  11920 loss value =  0.042546650574466674\n",
      "step =  11940 loss value =  0.04248817845663797\n",
      "step =  11960 loss value =  0.04242986578591541\n",
      "step =  11980 loss value =  0.04237171191413057\n",
      "step =  12000 loss value =  0.04231371619661149\n",
      "step =  12020 loss value =  0.042255877992162016\n",
      "step =  12040 loss value =  0.042198196663030464\n",
      "step =  12060 loss value =  0.042140671574890404\n",
      "step =  12080 loss value =  0.04208330209682715\n",
      "step =  12100 loss value =  0.04202608760129228\n",
      "step =  12120 loss value =  0.0419690274641053\n",
      "step =  12140 loss value =  0.04191212106441586\n",
      "step =  12160 loss value =  0.041855367784687195\n",
      "step =  12180 loss value =  0.04179876701067711\n",
      "step =  12200 loss value =  0.04174231813140537\n",
      "step =  12220 loss value =  0.04168602053914834\n",
      "step =  12240 loss value =  0.04162987362940207\n",
      "step =  12260 loss value =  0.0415738768008687\n",
      "step =  12280 loss value =  0.0415180294554359\n",
      "step =  12300 loss value =  0.04146233099815384\n",
      "step =  12320 loss value =  0.04140678083720981\n",
      "step =  12340 loss value =  0.04135137838391661\n",
      "step =  12360 loss value =  0.04129612305268528\n",
      "step =  12380 loss value =  0.04124101426101092\n",
      "step =  12400 loss value =  0.04118605142944485\n",
      "step =  12420 loss value =  0.04113123398157582\n",
      "step =  12440 loss value =  0.04107656134402031\n",
      "step =  12460 loss value =  0.04102203294639262\n",
      "step =  12480 loss value =  0.04096764822128746\n",
      "step =  12500 loss value =  0.04091340660425949\n",
      "step =  12520 loss value =  0.04085930753380711\n",
      "step =  12540 loss value =  0.04080535045134914\n",
      "step =  12560 loss value =  0.040751534801211686\n",
      "step =  12580 loss value =  0.04069786003060824\n",
      "step =  12600 loss value =  0.04064432558961889\n",
      "step =  12620 loss value =  0.0405909309311685\n",
      "step =  12640 loss value =  0.04053767551101134\n",
      "step =  12660 loss value =  0.04048455878771981\n",
      "step =  12680 loss value =  0.04043158022265042\n",
      "step =  12700 loss value =  0.04037873927994356\n",
      "step =  12720 loss value =  0.040326035426494204\n",
      "step =  12740 loss value =  0.040273468131941305\n",
      "step =  12760 loss value =  0.04022103686863819\n",
      "step =  12780 loss value =  0.040168741111653306\n",
      "step =  12800 loss value =  0.04011658033873374\n",
      "step =  12820 loss value =  0.040064554030304934\n",
      "step =  12840 loss value =  0.04001266166944575\n",
      "step =  12860 loss value =  0.03996090274186592\n",
      "step =  12880 loss value =  0.03990927673590127\n",
      "step =  12900 loss value =  0.03985778314248801\n",
      "step =  12920 loss value =  0.039806421455151494\n",
      "step =  12940 loss value =  0.03975519116998697\n",
      "step =  12960 loss value =  0.03970409178564402\n",
      "step =  12980 loss value =  0.039653122803311486\n",
      "step =  13000 loss value =  0.03960228372670317\n",
      "step =  13020 loss value =  0.03955157406203224\n",
      "step =  13040 loss value =  0.03950099331800526\n",
      "step =  13060 loss value =  0.0394505410058083\n",
      "step =  13080 loss value =  0.03940021663908464\n",
      "step =  13100 loss value =  0.03935001973391904\n",
      "step =  13120 loss value =  0.039299949808828394\n",
      "step =  13140 loss value =  0.03925000638474332\n",
      "step =  13160 loss value =  0.03920018898498971\n",
      "step =  13180 loss value =  0.039150497135279703\n",
      "step =  13200 loss value =  0.03910093036369557\n",
      "step =  13220 loss value =  0.03905148820067105\n",
      "step =  13240 loss value =  0.03900217017898029\n",
      "step =  13260 loss value =  0.038952975833721794\n",
      "step =  13280 loss value =  0.038903904702307666\n",
      "step =  13300 loss value =  0.03885495632444358\n",
      "step =  13320 loss value =  0.038806130242116474\n",
      "step =  13340 loss value =  0.03875742599958349\n",
      "step =  13360 loss value =  0.03870884314335487\n",
      "step =  13380 loss value =  0.03866038122218009\n",
      "step =  13400 loss value =  0.03861203978703887\n",
      "step =  13420 loss value =  0.038563818391119074\n",
      "step =  13440 loss value =  0.03851571658980733\n",
      "step =  13460 loss value =  0.03846773394067989\n",
      "step =  13480 loss value =  0.03841987000348075\n",
      "step =  13500 loss value =  0.03837212434011792\n",
      "step =  13520 loss value =  0.03832449651464285\n",
      "step =  13540 loss value =  0.03827698609323685\n",
      "step =  13560 loss value =  0.038229592644205404\n",
      "step =  13580 loss value =  0.03818231573795738\n",
      "step =  13600 loss value =  0.03813515494699993\n",
      "step =  13620 loss value =  0.03808810984591632\n",
      "step =  13640 loss value =  0.03804118001135937\n",
      "step =  13660 loss value =  0.03799436502204299\n",
      "step =  13680 loss value =  0.037947664458720214\n",
      "step =  13700 loss value =  0.03790107790417429\n",
      "step =  13720 loss value =  0.03785460494321404\n",
      "step =  13740 loss value =  0.03780824516264805\n",
      "step =  13760 loss value =  0.037761998151285815\n",
      "step =  13780 loss value =  0.03771586349991597\n",
      "step =  13800 loss value =  0.03766984080129724\n",
      "step =  13820 loss value =  0.037623929650152106\n",
      "step =  13840 loss value =  0.03757812964314237\n",
      "step =  13860 loss value =  0.03753244037887367\n",
      "step =  13880 loss value =  0.03748686145787377\n",
      "step =  13900 loss value =  0.03744139248257958\n",
      "step =  13920 loss value =  0.03739603305733464\n",
      "step =  13940 loss value =  0.03735078278837152\n",
      "step =  13960 loss value =  0.0373056412837965\n",
      "step =  13980 loss value =  0.03726060815358718\n",
      "step =  14000 loss value =  0.03721568300957851\n",
      "step =  14020 loss value =  0.03717086546545001\n",
      "step =  14040 loss value =  0.03712615513671315\n",
      "step =  14060 loss value =  0.03708155164070493\n",
      "step =  14080 loss value =  0.037037054596580084\n",
      "step =  14100 loss value =  0.036992663625285235\n",
      "step =  14120 loss value =  0.0369483783495687\n",
      "step =  14140 loss value =  0.03690419839395622\n",
      "step =  14160 loss value =  0.03686012338474206\n",
      "step =  14180 loss value =  0.036816152949981716\n",
      "step =  14200 loss value =  0.03677228671948366\n",
      "step =  14220 loss value =  0.03672852432478993\n",
      "step =  14240 loss value =  0.036684865399178604\n",
      "step =  14260 loss value =  0.036641309577643846\n",
      "step =  14280 loss value =  0.03659785649688641\n",
      "step =  14300 loss value =  0.036554505795311484\n",
      "step =  14320 loss value =  0.03651125711301224\n",
      "step =  14340 loss value =  0.03646811009176052\n",
      "step =  14360 loss value =  0.036425064374994785\n",
      "step =  14380 loss value =  0.03638211960782116\n",
      "step =  14400 loss value =  0.03633927543699275\n",
      "step =  14420 loss value =  0.036296531510900835\n",
      "step =  14440 loss value =  0.036253887479569466\n",
      "step =  14460 loss value =  0.036211342994646546\n",
      "step =  14480 loss value =  0.036168897709394014\n",
      "step =  14500 loss value =  0.036126551278673966\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step =  14520 loss value =  0.03608430335894257\n",
      "step =  14540 loss value =  0.03604215360824329\n",
      "step =  14560 loss value =  0.03600010168619241\n",
      "step =  14580 loss value =  0.0359581472539749\n",
      "step =  14600 loss value =  0.035916289974336144\n",
      "step =  14620 loss value =  0.03587452951156662\n",
      "step =  14640 loss value =  0.03583286553149481\n",
      "step =  14660 loss value =  0.03579129770149009\n",
      "step =  14680 loss value =  0.03574982569043375\n",
      "step =  14700 loss value =  0.03570844916872256\n",
      "step =  14720 loss value =  0.035667167808267265\n",
      "step =  14740 loss value =  0.035625981282467904\n",
      "step =  14760 loss value =  0.0355848892662095\n",
      "step =  14780 loss value =  0.0355438914358642\n",
      "step =  14800 loss value =  0.035502987469270796\n",
      "step =  14820 loss value =  0.03546217704573595\n",
      "step =  14840 loss value =  0.03542145984601381\n",
      "step =  14860 loss value =  0.035380835552311606\n",
      "step =  14880 loss value =  0.035340303848272026\n",
      "step =  14900 loss value =  0.03529986441896992\n",
      "step =  14920 loss value =  0.03525951695090034\n",
      "step =  14940 loss value =  0.03521926113197101\n",
      "step =  14960 loss value =  0.03517909665149737\n",
      "step =  14980 loss value =  0.0351390232001933\n",
      "step =  15000 loss value =  0.03509904047016543\n",
      "step =  15020 loss value =  0.035059148154899696\n",
      "step =  15040 loss value =  0.03501934594925901\n",
      "step =  15060 loss value =  0.03497963354947414\n",
      "step =  15080 loss value =  0.034940010653133714\n",
      "step =  15100 loss value =  0.03490047695917918\n",
      "step =  15120 loss value =  0.03486103216789522\n",
      "step =  15140 loss value =  0.03482167598090633\n",
      "step =  15160 loss value =  0.03478240810116642\n",
      "step =  15180 loss value =  0.03474322823294842\n",
      "step =  15200 loss value =  0.034704136081845034\n",
      "step =  15220 loss value =  0.0346651313547492\n",
      "step =  15240 loss value =  0.034626213759859054\n",
      "step =  15260 loss value =  0.03458738300667038\n",
      "step =  15280 loss value =  0.03454863880595791\n",
      "step =  15300 loss value =  0.03450998086977832\n",
      "step =  15320 loss value =  0.03447140891146221\n",
      "step =  15340 loss value =  0.03443292264559849\n",
      "step =  15360 loss value =  0.03439452178804024\n",
      "step =  15380 loss value =  0.03435620605589007\n",
      "step =  15400 loss value =  0.0343179751674956\n",
      "step =  15420 loss value =  0.03427982884244054\n",
      "step =  15440 loss value =  0.0342417668015367\n",
      "step =  15460 loss value =  0.03420378876682649\n",
      "step =  15480 loss value =  0.034165894461562964\n",
      "step =  15500 loss value =  0.03412808361021596\n",
      "step =  15520 loss value =  0.03409035593845505\n",
      "step =  15540 loss value =  0.03405271117314608\n",
      "step =  15560 loss value =  0.03401514904234674\n",
      "step =  15580 loss value =  0.03397766927530308\n",
      "step =  15600 loss value =  0.033940271602437025\n",
      "step =  15620 loss value =  0.0339029557553406\n",
      "step =  15640 loss value =  0.033865721466770415\n",
      "step =  15660 loss value =  0.033828568470647684\n",
      "step =  15680 loss value =  0.033791496502036826\n",
      "step =  15700 loss value =  0.033754505297160065\n",
      "step =  15720 loss value =  0.03371759459337097\n",
      "step =  15740 loss value =  0.0336807641291618\n",
      "step =  15760 loss value =  0.03364401364415335\n",
      "step =  15780 loss value =  0.03360734287908573\n",
      "step =  15800 loss value =  0.03357075157581771\n",
      "step =  15820 loss value =  0.03353423947731407\n",
      "step =  15840 loss value =  0.033497806327645895\n",
      "step =  15860 loss value =  0.03346145187198676\n",
      "step =  15880 loss value =  0.03342517585659375\n",
      "step =  15900 loss value =  0.03338897802881982\n",
      "step =  15920 loss value =  0.03335285813709502\n",
      "step =  15940 loss value =  0.03331681593091594\n",
      "step =  15960 loss value =  0.03328085116086508\n",
      "step =  15980 loss value =  0.033244963578573795\n",
      "step =  16000 loss value =  0.03320915293673783\n",
      "step =  16020 loss value =  0.03317341898910448\n",
      "step =  16040 loss value =  0.03313776149046811\n",
      "step =  16060 loss value =  0.03310218019666261\n",
      "step =  16080 loss value =  0.03306667486455856\n",
      "step =  16100 loss value =  0.03303124525205923\n",
      "step =  16120 loss value =  0.032995891118085485\n",
      "step =  16140 loss value =  0.032960612222585624\n",
      "step =  16160 loss value =  0.03292540832651617\n",
      "step =  16180 loss value =  0.03289027919184602\n",
      "step =  16200 loss value =  0.03285522458154254\n",
      "step =  16220 loss value =  0.03282024425957856\n",
      "step =  16240 loss value =  0.03278533799091324\n",
      "step =  16260 loss value =  0.03275050554149742\n",
      "step =  16280 loss value =  0.03271574667825925\n",
      "step =  16300 loss value =  0.032681061169109044\n",
      "step =  16320 loss value =  0.03264644878292823\n",
      "step =  16340 loss value =  0.032611909289565866\n",
      "step =  16360 loss value =  0.032577442459831345\n",
      "step =  16380 loss value =  0.03254304806549105\n",
      "step =  16400 loss value =  0.03250872587926467\n",
      "step =  16420 loss value =  0.032474475674817926\n",
      "step =  16440 loss value =  0.03244029722676161\n",
      "step =  16460 loss value =  0.03240619031064187\n",
      "step =  16480 loss value =  0.03237215470293601\n",
      "step =  16500 loss value =  0.03233819018104991\n",
      "step =  16520 loss value =  0.03230429652331142\n",
      "step =  16540 loss value =  0.032270473508970124\n",
      "step =  16560 loss value =  0.032236720918184046\n",
      "step =  16580 loss value =  0.03220303853202606\n",
      "step =  16600 loss value =  0.032169426132466664\n",
      "step =  16620 loss value =  0.03213588350237926\n",
      "step =  16640 loss value =  0.032102410425529304\n",
      "step =  16660 loss value =  0.032069006686574406\n",
      "step =  16680 loss value =  0.03203567207105996\n",
      "step =  16700 loss value =  0.03200240636540759\n",
      "step =  16720 loss value =  0.031969209356919895\n",
      "step =  16740 loss value =  0.03193608083376588\n",
      "step =  16760 loss value =  0.031903020584988125\n",
      "step =  16780 loss value =  0.0318700284004895\n",
      "step =  16800 loss value =  0.03183710407102872\n",
      "step =  16820 loss value =  0.03180424738822489\n",
      "step =  16840 loss value =  0.03177145814454135\n",
      "step =  16860 loss value =  0.03173873613329316\n",
      "step =  16880 loss value =  0.03170608114863163\n",
      "step =  16900 loss value =  0.03167349298554731\n",
      "step =  16920 loss value =  0.031640971439863955\n",
      "step =  16940 loss value =  0.03160851630823288\n",
      "step =  16960 loss value =  0.03157612738813069\n",
      "step =  16980 loss value =  0.03154380447785453\n",
      "step =  17000 loss value =  0.031511547376516666\n",
      "step =  17020 loss value =  0.031479355884041806\n",
      "step =  17040 loss value =  0.031447229801163636\n",
      "step =  17060 loss value =  0.03141516892941916\n",
      "step =  17080 loss value =  0.031383173071144146\n",
      "step =  17100 loss value =  0.03135124202947145\n",
      "step =  17120 loss value =  0.031319375608324804\n",
      "step =  17140 loss value =  0.031287573612418754\n",
      "step =  17160 loss value =  0.0312558358472473\n",
      "step =  17180 loss value =  0.031224162119087524\n",
      "step =  17200 loss value =  0.03119255223499298\n",
      "step =  17220 loss value =  0.031161006002786654\n",
      "step =  17240 loss value =  0.03112952323106289\n",
      "step =  17260 loss value =  0.031098103729178678\n",
      "step =  17280 loss value =  0.031066747307253145\n",
      "step =  17300 loss value =  0.031035453776161577\n",
      "step =  17320 loss value =  0.031004222947532176\n",
      "step =  17340 loss value =  0.030973054633741202\n",
      "step =  17360 loss value =  0.030941948647915087\n",
      "step =  17380 loss value =  0.030910904803917654\n",
      "step =  17400 loss value =  0.030879922916354514\n",
      "step =  17420 loss value =  0.030849002800565833\n",
      "step =  17440 loss value =  0.0308181442726207\n",
      "step =  17460 loss value =  0.030787347149317773\n",
      "step =  17480 loss value =  0.030756611248177183\n",
      "step =  17500 loss value =  0.030725936387445425\n",
      "step =  17520 loss value =  0.030695322386075634\n",
      "step =  17540 loss value =  0.030664769063744574\n",
      "step =  17560 loss value =  0.03063427624082935\n",
      "step =  17580 loss value =  0.030603843738423307\n",
      "step =  17600 loss value =  0.03057347137831436\n",
      "step =  17620 loss value =  0.030543158982994187\n",
      "step =  17640 loss value =  0.030512906375649056\n",
      "step =  17660 loss value =  0.030482713380158753\n",
      "step =  17680 loss value =  0.03045257982109097\n",
      "step =  17700 loss value =  0.030422505523699996\n",
      "step =  17720 loss value =  0.030392490313925723\n",
      "step =  17740 loss value =  0.030362534018381404\n",
      "step =  17760 loss value =  0.030332636464358076\n",
      "step =  17780 loss value =  0.030302797479823052\n",
      "step =  17800 loss value =  0.030273016893408933\n",
      "step =  17820 loss value =  0.03024329453441541\n",
      "step =  17840 loss value =  0.030213630232804493\n",
      "step =  17860 loss value =  0.0301840238192008\n",
      "step =  17880 loss value =  0.030154475124882764\n",
      "step =  17900 loss value =  0.030124983981780346\n",
      "step =  17920 loss value =  0.03009555022247706\n",
      "step =  17940 loss value =  0.03006617368020341\n",
      "step =  17960 loss value =  0.030036854188834887\n",
      "step =  17980 loss value =  0.03000759158288168\n",
      "step =  18000 loss value =  0.029978385697499735\n",
      "step =  18020 loss value =  0.02994923636847097\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step =  18040 loss value =  0.029920143432216027\n",
      "step =  18060 loss value =  0.029891106725781037\n",
      "step =  18080 loss value =  0.029862126086840374\n",
      "step =  18100 loss value =  0.029833201353688703\n",
      "step =  18120 loss value =  0.029804332365236726\n",
      "step =  18140 loss value =  0.02977551896101913\n",
      "step =  18160 loss value =  0.029746760981183094\n",
      "step =  18180 loss value =  0.02971805826648061\n",
      "step =  18200 loss value =  0.029689410658276176\n",
      "step =  18220 loss value =  0.029660817998539243\n",
      "step =  18240 loss value =  0.029632280129837995\n",
      "step =  18260 loss value =  0.029603796895344772\n",
      "step =  18280 loss value =  0.02957536813882447\n",
      "step =  18300 loss value =  0.029546993704640742\n",
      "step =  18320 loss value =  0.02951867343774075\n",
      "step =  18340 loss value =  0.02949040718366307\n",
      "step =  18360 loss value =  0.02946219478853416\n",
      "step =  18380 loss value =  0.029434036099057866\n",
      "step =  18400 loss value =  0.029405930962523592\n",
      "step =  18420 loss value =  0.02937787922679204\n",
      "step =  18440 loss value =  0.029349880740301523\n",
      "step =  18460 loss value =  0.02932193535206\n",
      "step =  18480 loss value =  0.029294042911646036\n",
      "step =  18500 loss value =  0.029266203269202488\n",
      "step =  18520 loss value =  0.0292384162754382\n",
      "step =  18540 loss value =  0.029210681781620503\n",
      "step =  18560 loss value =  0.02918299963957578\n",
      "step =  18580 loss value =  0.029155369701685377\n",
      "step =  18600 loss value =  0.029127791820887042\n",
      "step =  18620 loss value =  0.029100265850664053\n",
      "step =  18640 loss value =  0.029072791645051655\n",
      "step =  18660 loss value =  0.02904536905862812\n",
      "step =  18680 loss value =  0.02901799794651407\n",
      "step =  18700 loss value =  0.028990678164374246\n",
      "step =  18720 loss value =  0.028963409568410384\n",
      "step =  18740 loss value =  0.028936192015353347\n",
      "step =  18760 loss value =  0.028909025362472366\n",
      "step =  18780 loss value =  0.02888190946756426\n",
      "step =  18800 loss value =  0.028854844188958974\n",
      "step =  18820 loss value =  0.02882782938550231\n",
      "step =  18840 loss value =  0.028800864916575353\n",
      "step =  18860 loss value =  0.028773950642070247\n",
      "step =  18880 loss value =  0.028747086422401424\n",
      "step =  18900 loss value =  0.028720272118496645\n",
      "step =  18920 loss value =  0.02869350759179793\n",
      "step =  18940 loss value =  0.02866679270426033\n",
      "step =  18960 loss value =  0.02864012731834665\n",
      "step =  18980 loss value =  0.028613511297023907\n",
      "step =  19000 loss value =  0.02858694450376467\n",
      "step =  19020 loss value =  0.02856042680254749\n",
      "step =  19040 loss value =  0.028533958057840526\n",
      "step =  19060 loss value =  0.028507538134619214\n",
      "step =  19080 loss value =  0.028481166898347034\n",
      "step =  19100 loss value =  0.028454844214983715\n",
      "step =  19120 loss value =  0.02842856995098084\n",
      "step =  19140 loss value =  0.028402343973272185\n",
      "step =  19160 loss value =  0.028376166149279264\n",
      "step =  19180 loss value =  0.028350036346911252\n",
      "step =  19200 loss value =  0.028323954434555747\n",
      "step =  19220 loss value =  0.028297920281081756\n",
      "step =  19240 loss value =  0.028271933755832493\n",
      "step =  19260 loss value =  0.028245994728629004\n",
      "step =  19280 loss value =  0.02822010306976246\n",
      "step =  19300 loss value =  0.028194258649996477\n",
      "step =  19320 loss value =  0.02816846134056291\n",
      "step =  19340 loss value =  0.028142711013160536\n",
      "step =  19360 loss value =  0.02811700753994973\n",
      "step =  19380 loss value =  0.028091350793556905\n",
      "step =  19400 loss value =  0.028065740647066763\n",
      "step =  19420 loss value =  0.028040176974020427\n",
      "step =  19440 loss value =  0.028014659648419\n",
      "step =  19460 loss value =  0.027989188544716763\n",
      "step =  19480 loss value =  0.027963763537817134\n",
      "step =  19500 loss value =  0.027938384503073375\n",
      "step =  19520 loss value =  0.027913051316290228\n",
      "step =  19540 loss value =  0.027887763853715348\n",
      "step =  19560 loss value =  0.027862521992039525\n",
      "step =  19580 loss value =  0.027837325608399688\n",
      "step =  19600 loss value =  0.027812174580370465\n",
      "step =  19620 loss value =  0.027787068785962522\n",
      "step =  19640 loss value =  0.02776200810362408\n",
      "step =  19660 loss value =  0.02773699241223831\n",
      "step =  19680 loss value =  0.02771202159111903\n",
      "step =  19700 loss value =  0.027687095520012233\n",
      "step =  19720 loss value =  0.027662214079089205\n",
      "step =  19740 loss value =  0.027637377148950985\n",
      "step =  19760 loss value =  0.02761258461061986\n",
      "step =  19780 loss value =  0.027587836345545187\n",
      "step =  19800 loss value =  0.027563132235591425\n",
      "step =  19820 loss value =  0.027538472163045245\n",
      "step =  19840 loss value =  0.02751385601061153\n",
      "step =  19860 loss value =  0.027489283661407585\n",
      "step =  19880 loss value =  0.027464754998965984\n",
      "step =  19900 loss value =  0.027440269907228813\n",
      "step =  19920 loss value =  0.027415828270548757\n",
      "step =  19940 loss value =  0.027391429973689013\n",
      "step =  19960 loss value =  0.027367074901813757\n",
      "step =  19980 loss value =  0.027342762940496427\n",
      "step =  20000 loss value =  0.02731849397571212\n",
      "\n",
      "Elapsed Time =>  0:00:15.582290\n"
     ]
    }
   ],
   "source": [
    "obj1 = ClassificationTest(x_data, t_data, 1e-1, 20001)\n",
    "obj1.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ClassificationTest Object is created.\n",
      "\n",
      "Initial error value =  13.89220746760033 \n",
      " Initial W =  [[0.5488135 ]\n",
      " [0.71518937]] \n",
      " , b =  [0.60276338]\n",
      "step =  0 loss value =  15.314584792668047\n",
      "step =  5000 loss value =  0.08086980243290548\n",
      "step =  10000 loss value =  0.04901614492637792\n",
      "step =  15000 loss value =  0.03509904047016543\n",
      "step =  20000 loss value =  0.02731849397571212\n",
      "step =  25000 loss value =  0.022354589276717424\n",
      "step =  30000 loss value =  0.018914184399580227\n",
      "step =  35000 loss value =  0.0163899971830047\n",
      "step =  40000 loss value =  0.014459399163759898\n",
      "step =  45000 loss value =  0.012935219608862313\n",
      "step =  50000 loss value =  0.011701444673991042\n",
      "step =  55000 loss value =  0.010682353894443757\n",
      "step =  60000 loss value =  0.009826432422461473\n",
      "step =  65000 loss value =  0.009097418520998756\n",
      "step =  70000 loss value =  0.008469049414010638\n",
      "step =  75000 loss value =  0.007921838327751627\n",
      "step =  80000 loss value =  0.007441022300351464\n",
      "step =  85000 loss value =  0.007015213028542859\n",
      "step =  90000 loss value =  0.006635485004876121\n",
      "step =  95000 loss value =  0.006294744097219808\n",
      "step =  100000 loss value =  0.005987280872429566\n",
      "\n",
      "Elapsed Time =>  0:01:22.636763\n"
     ]
    }
   ],
   "source": [
    "obj2 = ClassificationTest(x_data, t_data, 1e-1, 100001)\n",
    "obj2.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "obj2.display_lossval_trend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "shapes (4,) and (2,1) not aligned: 4 (dim 0) != 2 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-83c99e98ba3e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mtest_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m17\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mobj2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-4-3917af3d9c38>\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, test_data)\u001b[0m\n\u001b[0;32m     77\u001b[0m     \u001b[1;31m# 입력변수 x : numpy type\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     78\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 79\u001b[1;33m         \u001b[0mz\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mW\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     80\u001b[0m         \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msigmoid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mz\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     81\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mdot\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: shapes (4,) and (2,1) not aligned: 4 (dim 0) != 2 (dim 0)"
     ]
    }
   ],
   "source": [
    "test_data = np.array([3, 17, 5, 10])\n",
    "obj2.predict(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([7.26808809e-05]), 0)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data = np.array([5, 8])\n",
    "obj2.predict(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1.]), 1)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data = np.array([7, 21])\n",
    "obj2.predict(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.36809255]), 0)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data = np.array([12, 0])\n",
    "obj2.predict(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ClassificationTest Object is created.\n",
      "\n",
      "Initial error value =  3.049207322500292 \n",
      " Initial W =  [[0.0871293]\n",
      " [0.0202184]] \n",
      " , b =  [0.83261985]\n",
      "step =  0 loss value =  20.294767435720903\n",
      "step =  1000 loss value =  0.1664047557740782\n",
      "step =  2000 loss value =  0.13174418052268763\n",
      "step =  3000 loss value =  0.10901501706899308\n",
      "step =  4000 loss value =  0.0929194370363145\n",
      "step =  5000 loss value =  0.08092532422604805\n",
      "step =  6000 loss value =  0.0716474108396057\n",
      "step =  7000 loss value =  0.06426081949434419\n",
      "step =  8000 loss value =  0.058243341896864594\n",
      "step =  9000 loss value =  0.053248333629927945\n",
      "step =  10000 loss value =  0.049036743350104\n",
      "step =  11000 loss value =  0.045438441857560755\n",
      "step =  12000 loss value =  0.04232909785826624\n",
      "step =  13000 loss value =  0.03961576814699589\n",
      "step =  14000 loss value =  0.03722759949522505\n",
      "step =  15000 loss value =  0.03510964649339493\n",
      "step =  16000 loss value =  0.03321865266959098\n",
      "step =  17000 loss value =  0.03152010477717856\n",
      "step =  18000 loss value =  0.029986133927423428\n",
      "step =  19000 loss value =  0.028593992820853762\n",
      "step =  20000 loss value =  0.02732493285805268\n",
      "step =  21000 loss value =  0.026163363884504146\n",
      "step =  22000 loss value =  0.02509621700008191\n",
      "step =  23000 loss value =  0.024112455453806796\n",
      "step =  24000 loss value =  0.02320269500946227\n",
      "step =  25000 loss value =  0.022358906252499425\n",
      "step =  26000 loss value =  0.021574178940119627\n",
      "step =  27000 loss value =  0.020842533829114402\n",
      "step =  28000 loss value =  0.020158771194248526\n",
      "step =  29000 loss value =  0.019518347960764022\n",
      "step =  30000 loss value =  0.01891727734295188\n",
      "step =  31000 loss value =  0.01835204632578213\n",
      "step =  32000 loss value =  0.017819547397993445\n",
      "step =  33000 loss value =  0.017317021747785898\n",
      "step =  34000 loss value =  0.01684201173856277\n",
      "step =  35000 loss value =  0.01639232094410309\n",
      "step =  36000 loss value =  0.01596598037765904\n",
      "step =  37000 loss value =  0.015561219824032111\n",
      "step =  38000 loss value =  0.015176443397658577\n",
      "step =  39000 loss value =  0.01481020861791557\n",
      "step =  40000 loss value =  0.014461208425407754\n",
      "step =  41000 loss value =  0.014128255668234406\n",
      "step =  42000 loss value =  0.013810269671670743\n",
      "step =  43000 loss value =  0.01350626457212848\n",
      "step =  44000 loss value =  0.013215339150965238\n",
      "step =  45000 loss value =  0.012936667948070692\n",
      "step =  46000 loss value =  0.012669493471174145\n",
      "step =  47000 loss value =  0.012413119346594993\n",
      "step =  48000 loss value =  0.012166904281306557\n",
      "step =  49000 loss value =  0.01193025672655279\n",
      "step =  50000 loss value =  0.011702630149647711\n",
      "step =  51000 loss value =  0.011483518834717701\n",
      "step =  52000 loss value =  0.01127245414462097\n",
      "step =  53000 loss value =  0.011069001186065582\n",
      "step =  54000 loss value =  0.010872755828091657\n",
      "step =  55000 loss value =  0.010683342031020488\n",
      "step =  56000 loss value =  0.01050040944881533\n",
      "step =  57000 loss value =  0.01032363127276818\n",
      "step =  58000 loss value =  0.010152702288641006\n",
      "step =  59000 loss value =  0.009987337123044402\n",
      "step =  60000 loss value =  0.009827268657874848\n",
      "step =  61000 loss value =  0.00967224659434802\n",
      "step =  62000 loss value =  0.009522036150396164\n",
      "step =  63000 loss value =  0.009376416877273043\n",
      "step =  64000 loss value =  0.009235181582704349\n",
      "step =  65000 loss value =  0.009098135349713602\n",
      "step =  66000 loss value =  0.008965094641247144\n",
      "step =  67000 loss value =  0.008835886482025494\n",
      "step =  68000 loss value =  0.00871034770993785\n",
      "step =  69000 loss value =  0.008588324290197843\n",
      "step =  70000 loss value =  0.008469670686205383\n",
      "step =  71000 loss value =  0.00835424928172052\n",
      "step =  72000 loss value =  0.008241929849525344\n",
      "step =  73000 loss value =  0.008132589062281158\n",
      "step =  74000 loss value =  0.008026110041726403\n",
      "step =  75000 loss value =  0.007922381942736017\n",
      "step =  76000 loss value =  0.007821299569163944\n",
      "step =  77000 loss value =  0.007722763018640366\n",
      "step =  78000 loss value =  0.007626677353869781\n",
      "step =  79000 loss value =  0.007532952298049148\n",
      "step =  80000 loss value =  0.007441501952493928\n",
      "step =  81000 loss value =  0.007352244534479706\n",
      "step =  82000 loss value =  0.007265102133749077\n",
      "step =  83000 loss value =  0.007180000486058966\n",
      "step =  84000 loss value =  0.007096868762443106\n",
      "step =  85000 loss value =  0.00701563937291427\n",
      "step =  86000 loss value =  0.006936247783475388\n",
      "step =  87000 loss value =  0.006858632345400773\n",
      "step =  88000 loss value =  0.006782734135804995\n",
      "step =  89000 loss value =  0.006708496808704711\n",
      "step =  90000 loss value =  0.0066358664556963\n",
      "step =  91000 loss value =  0.00656479147560913\n",
      "step =  92000 loss value =  0.006495222452406567\n",
      "step =  93000 loss value =  0.006427112040766706\n",
      "step =  94000 loss value =  0.006360414858779005\n",
      "step =  95000 loss value =  0.00629508738723977\n",
      "step =  96000 loss value =  0.00623108787508599\n",
      "step =  97000 loss value =  0.006168376250520579\n",
      "step =  98000 loss value =  0.006106914037447457\n",
      "step =  99000 loss value =  0.0060466642768561485\n",
      "step =  100000 loss value =  0.005987591452795724\n",
      "step =  101000 loss value =  0.005929661422645861\n",
      "step =  102000 loss value =  0.0058728413513922184\n",
      "step =  103000 loss value =  0.00581709964963313\n",
      "step =  104000 loss value =  0.005762405915084629\n",
      "step =  105000 loss value =  0.005708730877349946\n",
      "step =  106000 loss value =  0.00565604634574716\n",
      "step =  107000 loss value =  0.005604325159986069\n",
      "step =  108000 loss value =  0.005553541143525885\n",
      "step =  109000 loss value =  0.005503669059453731\n",
      "step =  110000 loss value =  0.0054546845687082745\n",
      "step =  111000 loss value =  0.0054065641905065576\n",
      "step =  112000 loss value =  0.005359285264855237\n",
      "step =  113000 loss value =  0.005312825917003628\n",
      "step =  114000 loss value =  0.005267165023733569\n",
      "step =  115000 loss value =  0.005222282181359085\n",
      "step =  116000 loss value =  0.005178157675360862\n",
      "step =  117000 loss value =  0.005134772451537407\n",
      "step =  118000 loss value =  0.005092108088591889\n",
      "step =  119000 loss value =  0.005050146772080731\n",
      "step =  120000 loss value =  0.0050088712696316155\n",
      "step =  121000 loss value =  0.0049682649073626815\n",
      "step =  122000 loss value =  0.004928311547451173\n",
      "step =  123000 loss value =  0.004888995566761475\n",
      "step =  124000 loss value =  0.004850301836496691\n",
      "step =  125000 loss value =  0.0048122157027998234\n",
      "step =  126000 loss value =  0.004774722968267214\n",
      "step =  127000 loss value =  0.00473780987431771\n",
      "step =  128000 loss value =  0.00470146308436992\n",
      "step =  129000 loss value =  0.004665669667787345\n",
      "step =  130000 loss value =  0.004630417084559694\n",
      "step =  131000 loss value =  0.004595693170653112\n",
      "step =  132000 loss value =  0.004561486124038866\n",
      "step =  133000 loss value =  0.004527784491322164\n",
      "step =  134000 loss value =  0.004494577154965707\n",
      "step =  135000 loss value =  0.00446185332107378\n",
      "step =  136000 loss value =  0.004429602507696638\n",
      "step =  137000 loss value =  0.00439781453365047\n",
      "step =  138000 loss value =  0.004366479507805565\n",
      "step =  139000 loss value =  0.004335587818829807\n",
      "step =  140000 loss value =  0.00430513012536781\n",
      "step =  141000 loss value =  0.004275097346631334\n",
      "step =  142000 loss value =  0.0042454806533759265\n",
      "step =  143000 loss value =  0.0042162714592526435\n",
      "step =  144000 loss value =  0.004187461412513687\n",
      "step =  145000 loss value =  0.004159042388056735\n",
      "step =  146000 loss value =  0.004131006479785981\n",
      "step =  147000 loss value =  0.004103345993286886\n",
      "step =  148000 loss value =  0.004076053438792844\n",
      "step =  149000 loss value =  0.00404912152441926\n",
      "step =  150000 loss value =  0.004022543149679871\n",
      "step =  151000 loss value =  0.003996311399243253\n",
      "step =  152000 loss value =  0.003970419536939801\n",
      "step =  153000 loss value =  0.0039448609999992446\n",
      "step =  154000 loss value =  0.003919629393505956\n",
      "step =  155000 loss value =  0.0038947184850658526\n",
      "step =  156000 loss value =  0.003870122199681434\n",
      "step =  157000 loss value =  0.003845834614807608\n",
      "step =  158000 loss value =  0.003821849955606164\n",
      "step =  159000 loss value =  0.0037981625903696446\n",
      "step =  160000 loss value =  0.0037747670261099485\n",
      "step =  161000 loss value =  0.0037516579043188098\n",
      "step =  162000 loss value =  0.0037288299968763305\n",
      "step =  163000 loss value =  0.003706278202109698\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step =  164000 loss value =  0.003683997540993573\n",
      "step =  165000 loss value =  0.0036619831534839393\n",
      "step =  166000 loss value =  0.0036402302949917462\n",
      "step =  167000 loss value =  0.0036187343329711747\n",
      "step =  168000 loss value =  0.003597490743632938\n",
      "step =  169000 loss value =  0.0035764951087693397\n",
      "step =  170000 loss value =  0.0035557431126968116\n",
      "step =  171000 loss value =  0.003535230539296859\n",
      "step =  172000 loss value =  0.0035149532691610076\n",
      "step =  173000 loss value =  0.0034949072768346097\n",
      "step =  174000 loss value =  0.0034750886281554403\n",
      "step =  175000 loss value =  0.003455493477679082\n",
      "step =  176000 loss value =  0.003436118066189911\n",
      "step =  177000 loss value =  0.0034169587183038655\n",
      "step =  178000 loss value =  0.0033980118401396218\n",
      "step =  179000 loss value =  0.0033792739170727384\n",
      "step =  180000 loss value =  0.0033607415115656904\n",
      "step =  181000 loss value =  0.003342411261064852\n",
      "step =  182000 loss value =  0.003324279875965303\n",
      "step =  183000 loss value =  0.003306344137647084\n",
      "step =  184000 loss value =  0.003288600896566794\n",
      "step =  185000 loss value =  0.00327104707041848\n",
      "step =  186000 loss value =  0.0032536796423460745\n",
      "step =  187000 loss value =  0.0032364956592178845\n",
      "step =  188000 loss value =  0.0032194922299508336\n",
      "step =  189000 loss value =  0.0032026665238915376\n",
      "step =  190000 loss value =  0.0031860157692445434\n",
      "step =  191000 loss value =  0.0031695372515500577\n",
      "step =  192000 loss value =  0.003153228312211315\n",
      "step =  193000 loss value =  0.0031370863470625452\n",
      "step =  194000 loss value =  0.003121108804985875\n",
      "step =  195000 loss value =  0.0031052931865634447\n",
      "step =  196000 loss value =  0.0030896370427793097\n",
      "step =  197000 loss value =  0.003074137973751146\n",
      "step =  198000 loss value =  0.0030587936275081147\n",
      "step =  199000 loss value =  0.003043601698797559\n",
      "step =  200000 loss value =  0.0030285599279354083\n",
      "\n",
      "Elapsed Time =>  0:01:42.019120\n"
     ]
    }
   ],
   "source": [
    "obj5 = ClassificationTest(x_data, t_data, 1e-1, 200001)\n",
    "obj5.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.27660511]), 0)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data = np.array([12, 0])\n",
    "obj5.predict(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([7.19576463e-18]), 0)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data = np.array([1, 1])\n",
    "obj5.predict(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([2.15526478e-15]), 0)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data = np.array([2, 2])\n",
    "obj5.predict(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.99999284]), 1)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data = np.array([10, 10])\n",
    "obj5.predict(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.9999586]), 1)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data = np.array([15, 0])\n",
    "obj5.predict(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.9997808]), 1)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data = np.array([14, 1])\n",
    "obj5.predict(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.99835376]), 1)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data = np.array([14, 0])\n",
    "obj5.predict(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.93837738]), 1)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data = np.array([13, 0])\n",
    "obj5.predict(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([2.40244396e-20]), 0)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data = np.array([0, 0])\n",
    "obj5.predict(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([6.05374682e-06]), 0)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data = np.array([9, 0])\n",
    "obj5.predict(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.00024103]), 0)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data = np.array([10, 0])\n",
    "obj5.predict(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.00951004]), 0)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data = np.array([11, 0])\n",
    "obj5.predict(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.27660511]), 0)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data = np.array([12, 0])\n",
    "obj5.predict(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.74198749]), 1)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data = np.array([12, 1])\n",
    "obj5.predict(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.00026311]), 0)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data = np.array([4, 11])\n",
    "obj5.predict(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[3.68448758],\n",
       "        [2.01768868]]),\n",
       " array([-45.17521533]))"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obj5.get_W_b()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ClassificationTest Object is created.\n",
      "\n",
      "Initial error value =  15.534121962505818 \n",
      " Initial W =  [[0.43758721]\n",
      " [0.891773  ]] \n",
      " , b =  [0.96366276]\n",
      "step =  0 loss value =  13.507142276132457\n",
      "step =  1000 loss value =  0.9872453685855429\n",
      "step =  2000 loss value =  0.6767327560204387\n",
      "step =  3000 loss value =  0.5405678897833517\n",
      "step =  4000 loss value =  0.4623542646387213\n",
      "step =  5000 loss value =  0.4104787809731492\n",
      "step =  6000 loss value =  0.37287400231722206\n",
      "step =  7000 loss value =  0.3439323619363839\n",
      "step =  8000 loss value =  0.3206866124503806\n",
      "step =  9000 loss value =  0.30141384580012004\n",
      "step =  10000 loss value =  0.2850413129273148\n",
      "step =  11000 loss value =  0.27086373275715137\n",
      "step =  12000 loss value =  0.25839679259257603\n",
      "step =  13000 loss value =  0.24729588460859028\n",
      "step =  14000 loss value =  0.23730845394088212\n",
      "step =  15000 loss value =  0.22824473136966883\n",
      "step =  16000 loss value =  0.21995903834053582\n",
      "step =  17000 loss value =  0.2123374372965427\n",
      "step =  18000 loss value =  0.20528933414254924\n",
      "step =  19000 loss value =  0.1987416236082028\n",
      "step =  20000 loss value =  0.19263451838162957\n",
      "\n",
      "Elapsed Time =>  0:00:12.715757\n"
     ]
    }
   ],
   "source": [
    "obj3 = ClassificationTest(x_data, t_data, 1e-2, 20001)\n",
    "obj3.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ClassificationTest Object is created.\n",
      "\n",
      "Initial error value =  15.49958292849021 \n",
      " Initial W =  [[0.56804456]\n",
      " [0.92559664]] \n",
      " , b =  [0.07103606]\n",
      "step =  0 loss value =  34.99999982628222\n",
      "step =  1000 loss value =  34.99999982628222\n",
      "step =  2000 loss value =  34.99999982628222\n",
      "step =  3000 loss value =  34.99999982628222\n",
      "step =  4000 loss value =  34.99999982628222\n",
      "step =  5000 loss value =  34.99999982628222\n",
      "step =  6000 loss value =  34.99999982628222\n",
      "step =  7000 loss value =  34.99999982628222\n",
      "step =  8000 loss value =  34.99999982628222\n",
      "step =  9000 loss value =  34.99999982628222\n",
      "step =  10000 loss value =  34.99999982628222\n",
      "step =  11000 loss value =  34.99999982628222\n",
      "step =  12000 loss value =  34.99999982628222\n",
      "step =  13000 loss value =  34.99999982628222\n",
      "step =  14000 loss value =  34.99999982628222\n",
      "step =  15000 loss value =  34.99999982628222\n",
      "step =  16000 loss value =  34.99999982628222\n",
      "step =  17000 loss value =  34.99999982628222\n",
      "step =  18000 loss value =  34.99999982628222\n",
      "step =  19000 loss value =  34.99999982628222\n",
      "step =  20000 loss value =  34.99999982628222\n",
      "\n",
      "Elapsed Time =>  0:00:11.956175\n"
     ]
    }
   ],
   "source": [
    "obj4 = ClassificationTest(x_data, t_data, 1, 20001)\n",
    "obj4.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ClassificationTest Object is created.\n",
      "\n",
      "Initial error value =  18.241460897668553 \n",
      " Initial W =  [[0.77815675]\n",
      " [0.87001215]] \n",
      " , b =  [0.97861834]\n",
      "step =  0 loss value =  18.04772127356661\n",
      "step =  1000 loss value =  2.0303527845547364\n",
      "step =  2000 loss value =  1.812111840989692\n",
      "step =  3000 loss value =  1.6340158091039247\n",
      "step =  4000 loss value =  1.4878813685034062\n",
      "step =  5000 loss value =  1.3669171894400574\n",
      "step =  6000 loss value =  1.2657487028647698\n",
      "step =  7000 loss value =  1.1802222789936567\n",
      "step =  8000 loss value =  1.1071565404343258\n",
      "step =  9000 loss value =  1.0441149142060764\n",
      "step =  10000 loss value =  0.9892216254643427\n",
      "step =  11000 loss value =  0.941020993173654\n",
      "step =  12000 loss value =  0.8983726189197524\n",
      "step =  13000 loss value =  0.8603741184715124\n",
      "step =  14000 loss value =  0.8263042448437403\n",
      "step =  15000 loss value =  0.7955808719480049\n",
      "step =  16000 loss value =  0.7677297544948752\n",
      "step =  17000 loss value =  0.7423611118225305\n",
      "step =  18000 loss value =  0.7191519195124366\n",
      "step =  19000 loss value =  0.6978323937554847\n",
      "step =  20000 loss value =  0.6781755804135893\n",
      "step =  21000 loss value =  0.6599892629546898\n",
      "step =  22000 loss value =  0.6431096176725737\n",
      "step =  23000 loss value =  0.6273961971240246\n",
      "step =  24000 loss value =  0.61272793197036\n",
      "step =  25000 loss value =  0.5989999202371536\n",
      "step =  26000 loss value =  0.5861208303227063\n",
      "step =  27000 loss value =  0.574010786098654\n",
      "step =  28000 loss value =  0.5625996334919102\n",
      "step =  29000 loss value =  0.5518255110640244\n",
      "step =  30000 loss value =  0.5416336644684823\n",
      "step =  31000 loss value =  0.5319754578048406\n",
      "step =  32000 loss value =  0.522807544903601\n",
      "step =  33000 loss value =  0.5140911712650506\n",
      "step =  34000 loss value =  0.505791583319799\n",
      "step =  35000 loss value =  0.49787752630507565\n",
      "step =  36000 loss value =  0.49032081567431823\n",
      "step =  37000 loss value =  0.4830959698127386\n",
      "step =  38000 loss value =  0.4761798940948463\n",
      "step =  39000 loss value =  0.4695516081238805\n",
      "step =  40000 loss value =  0.4631920094386891\n",
      "step =  41000 loss value =  0.45708366813807644\n",
      "step =  42000 loss value =  0.4512106478153138\n",
      "step =  43000 loss value =  0.4455583489620742\n",
      "step =  44000 loss value =  0.44011337162740477\n",
      "step =  45000 loss value =  0.4348633946313285\n",
      "step =  46000 loss value =  0.4297970690560756\n",
      "step =  47000 loss value =  0.42490392408834704\n",
      "step =  48000 loss value =  0.42017428357691056\n",
      "step =  49000 loss value =  0.4155991919123217\n",
      "step =  50000 loss value =  0.41117034803846686\n",
      "step =  51000 loss value =  0.4068800465758686\n",
      "step =  52000 loss value =  0.4027211251800908\n",
      "step =  53000 loss value =  0.3986869173797459\n",
      "step =  54000 loss value =  0.39477121024121903\n",
      "step =  55000 loss value =  0.39096820629455536\n",
      "step =  56000 loss value =  0.3872724892292164\n",
      "step =  57000 loss value =  0.3836789929320747\n",
      "step =  58000 loss value =  0.3801829734944263\n",
      "step =  59000 loss value =  0.3767799838615844\n",
      "step =  60000 loss value =  0.3734658508390181\n",
      "step =  61000 loss value =  0.37023665420368196\n",
      "step =  62000 loss value =  0.36708870769936525\n",
      "step =  63000 loss value =  0.3640185417210443\n",
      "step =  64000 loss value =  0.36102288751582506\n",
      "step =  65000 loss value =  0.3580986627479549\n",
      "step =  66000 loss value =  0.355242958292565\n",
      "step =  67000 loss value =  0.3524530261379749\n",
      "step =  68000 loss value =  0.3497262682895533\n",
      "step =  69000 loss value =  0.347060226579803\n",
      "step =  70000 loss value =  0.3444525732995418\n",
      "step =  71000 loss value =  0.34190110257405626\n",
      "step =  72000 loss value =  0.33940372241605143\n",
      "step =  73000 loss value =  0.33695844739425823\n",
      "step =  74000 loss value =  0.3345633918627522\n",
      "step =  75000 loss value =  0.33221676370164993\n",
      "step =  76000 loss value =  0.32991685852460045\n",
      "step =  77000 loss value =  0.32766205431303497\n",
      "step =  78000 loss value =  0.325450806440883\n",
      "step =  79000 loss value =  0.3232816430569995\n",
      "step =  80000 loss value =  0.32115316079565615\n",
      "\n",
      "Elapsed Time =>  0:00:46.002650\n"
     ]
    }
   ],
   "source": [
    "obj6 = ClassificationTest(x_data, t_data, 1e-3, 80001)\n",
    "obj6.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
